{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b33032-8cbd-4fa6-8ec2-1e59aff88f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from plotnine import (\n",
    "    ggplot,\n",
    "    geom_point, \n",
    "    geom_histogram, \n",
    "    geom_line,\n",
    "    geom_ribbon,\n",
    "    qplot, \n",
    "    coord_fixed, \n",
    "    aes, \n",
    "    facet_wrap, \n",
    "    labs,\n",
    "    scale_x_log10,\n",
    "    scale_y_log10\n",
    ")\n",
    "import polars as pl\n",
    "import torch\n",
    "\n",
    "from tokengrams import MemmapIndex, InMemoryIndex\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "import zstandard as zstd\n",
    "\n",
    "\n",
    "from ngram_markov.utils import create_ngrams, nanogpt_to_hooked_transformer_config, convert_nanogpt_weights\n",
    "from ngram_markov.model import GPT, GPTConfig\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import rustworkx as rx\n",
    "\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e049bbd-35f1-4dbc-a870-1c5e30155526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def noise_operator(tokens, unigram_probabilities, rho: float, n_gram: int, num_samples: int):\n",
    "    batch_size, seq_len = tokens.shape\n",
    "    device = tokens.device\n",
    "    \n",
    "    # Create a mask for the positions we'll potentially modify\n",
    "    mask = torch.ones((batch_size, seq_len), dtype=torch.bool, device=device)\n",
    "    mask[:, -n_gram:] = False\n",
    "    \n",
    "    # Expand tokens for num_samples\n",
    "    noised_tokens = tokens.unsqueeze(1).expand(-1, num_samples, -1).clone()\n",
    "    \n",
    "    # Create Bernoulli mask\n",
    "    bernoulli_mask = torch.bernoulli(torch.full((batch_size, num_samples, seq_len), rho, device=device))\n",
    "    bernoulli_mask = bernoulli_mask * mask.unsqueeze(1)\n",
    "    \n",
    "    # Sample from unigram distribution\n",
    "    sampled_tokens = torch.multinomial(\n",
    "        unigram_probabilities,\n",
    "        batch_size * num_samples * seq_len,\n",
    "        replacement=True\n",
    "    ).view(batch_size, num_samples, seq_len)\n",
    "    \n",
    "    # Apply noise\n",
    "    noised_tokens = torch.where(bernoulli_mask.bool(), sampled_tokens, noised_tokens)\n",
    "    \n",
    "    return noised_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd5c32-10f1-4035-b36c-e168cf77ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer/tinystories512\")\n",
    "ts_bin_path = 'data/tinystories/train.bin'\n",
    "index_path = \"data/tinystories/ngrams/suffix_tree.idx\"\n",
    "\n",
    "index = MemmapIndex(ts_bin_path, index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79802c-5356-42ed-8241-1b7bfcb18342",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.count([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab585ac-7674-4061-bf53-f045106ef89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 53_000\n",
    "model_path = Path('/media/External01/ngram-checkpoints/4layer_tinystories')\n",
    "\n",
    "#ckpt = torch.load(, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c77651-2563-4443-8f5f-0ca0ad26e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram_markov.model import GPT, GPTConfig\n",
    "from ngram_markov.utils import create_ngrams, nanogpt_to_hooked_transformer_config, convert_nanogpt_weights\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "import einops\n",
    "import torch\n",
    "import plotly.express as px\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "\n",
    "\n",
    "def load_nnsight_model(path):\n",
    "    ckpt = torch.load(path, map_location='cpu')\n",
    "    config = nanogpt_to_hooked_transformer_config(ckpt['model_args'])\n",
    "    tl_weights = convert_nanogpt_weights(ckpt['model'], config)\n",
    "    tl_model = HookedTransformer(config)\n",
    "    tl_model.load_state_dict(tl_weights)\n",
    "    return  LanguageModel(tl_model)\n",
    "\n",
    "\n",
    "def load_tl_model(path):\n",
    "    ckpt = torch.load(path, map_location='cpu')\n",
    "    config = nanogpt_to_hooked_transformer_config(ckpt['model_args'])\n",
    "    tl_weights = convert_nanogpt_weights(ckpt['model'], config)\n",
    "    tl_model = HookedTransformer(config)\n",
    "    tl_model.load_state_dict(tl_weights)\n",
    "    return tl_model\n",
    "\n",
    "model = load_tl_model(model_path / f'ckpt{epoch}.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3acf43-8650-4d49-9ef4-dadb2f9b6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd13149-c4bc-4461-ac57-6a42731c6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram_markov.model import GPT, GPTConfig\n",
    "from ngram_markov.utils import create_ngrams, nanogpt_to_hooked_transformer_config, convert_nanogpt_weights\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#gpt_config = GPTConfig(**ckpt['model_args'])\n",
    "\n",
    "#gpt_model = GPT(gpt_config)\n",
    "#gpt_model.load_state_dict(ckpt['model'])\n",
    "#gpt_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae91c2-1bde-4789-a988-3e8e42ca27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_local_attention_mask(seq_len: int, window_size: int, batch_size: int = 1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates a mask for local attention where each token can only attend to\n",
    "    the previous window_size tokens and itself, while maintaining causality.\n",
    "    \n",
    "    Args:\n",
    "    seq_len (int): The sequence length.\n",
    "    window_size (int): The number of previous tokens to attend to.\n",
    "    batch_size (int): The batch size.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: A boolean mask of shape (batch_size, 1, seq_len, seq_len).\n",
    "    \"\"\"\n",
    "    # Create a causal mask\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    \n",
    "    # Create a local attention mask\n",
    "    local_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1-window_size)\n",
    "    \n",
    "    # Combine causal and local masks\n",
    "    mask = causal_mask * local_mask\n",
    "    \n",
    "    # Add batch and head dimensions\n",
    "    mask = mask.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, seq_len, seq_len)\n",
    "    \n",
    "    return mask.bool()\n",
    "\n",
    "# Example usage:\n",
    "# seq_len = 1024\n",
    "# window_size = 256\n",
    "# batch_size = 1\n",
    "# local_mask = create_local_attention_mask(seq_len, window_size, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef4a14-6031-434f-85f1-65d526d00cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821be44c-2dab-4d35-b196-2e77cd568166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram_markov.ngrams import create_ngrams, calculate_ngram_kl_divergence\n",
    "\n",
    "\n",
    "data_dir = Path('data/tinystories')\n",
    "\n",
    "batch_size = 16\n",
    "block_size = 256\n",
    "device_type = 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(data_dir / 'train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(data_dir / 'validation.bin', dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x\n",
    "    else:\n",
    "        x = x.to(device)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe708c-f04b-461f-bf0c-5b9e45365811",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe73ac4-284b-441d-a6da-892c35cc30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_mask = create_local_attention_mask(block_size, 2, 16)\n",
    "full_preds = model(batch)\n",
    "local_preds = model(batch, attention_mask=local_mask)\n",
    "(full_preds - local.preds).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7316b80-7382-4a4b-871c-26bcd5fe9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(batch, attention_mask=local_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23add718-5d60-4077-bee8-7b8d3fcda54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -8.0185,     -inf,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [  0.7006,  -4.3057,     -inf,  ...,     -inf,     -inf,     -inf],\n",
       "        [  4.8513,   4.9111,   2.6494,  ...,     -inf,     -inf,     -inf],\n",
       "        ...,\n",
       "        [ -8.5765,  -8.0351,  -7.5239,  ...,   1.6409,     -inf,     -inf],\n",
       "        [ -9.4877, -10.9333,  -8.7445,  ...,   5.5819,   0.2017,     -inf],\n",
       "        [ -3.8308,  -4.4622,  -2.9396,  ...,   7.0039,   6.8585,   3.2335]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['attn_scores', 1][0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1653a666-12be-40e7-ad7c-381fbf623ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [[i] for i in range(512)]\n",
    "bigram_counts = torch.tensor(index.batch_count_next(queries, 512), dtype=torch.float32)\n",
    "unigrams = bigram_counts.sum(dim=1)\n",
    "unigrams /= unigrams.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f48c7184-4516-470a-8575-8e526088d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, _ = get_batch('train')\n",
    "\n",
    "\n",
    "\n",
    "noised_batch = noise_operator(batch, unigrams, 0.1, 4, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0a73f5e-c6db-4a1e-a90f-b9f7f5fa9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_minibatch(model, minibatch):\n",
    "    with torch.amp.autocast('cuda'):\n",
    "        logits = model(minibatch.to('cuda'))[:, -1:, :]\n",
    "        current_log_probs = log_softmax(logits, dim=-1)\n",
    "    return current_log_probs.to('cpu')\n",
    "    \n",
    "\n",
    "def noise_operator(tokens, unigram_probabilities, rho: float, n_gram: int, num_samples: int):\n",
    "    batch_size, seq_len = tokens.shape\n",
    "    device = tokens.device\n",
    "    \n",
    "    mask = torch.ones((batch_size, seq_len), dtype=torch.bool, device=device)\n",
    "    mask[:, -n_gram:] = False\n",
    "    \n",
    "    noised_tokens = tokens.unsqueeze(1).expand(-1, num_samples, -1).clone()\n",
    "    \n",
    "    bernoulli_mask = torch.bernoulli(torch.full((batch_size, num_samples, seq_len), rho, device=device))\n",
    "    bernoulli_mask = bernoulli_mask * mask.unsqueeze(1)\n",
    "    \n",
    "    sampled_tokens = torch.multinomial(\n",
    "        unigram_probabilities,\n",
    "        batch_size * num_samples * seq_len,\n",
    "        replacement=True\n",
    "    ).view(batch_size, num_samples, seq_len)\n",
    "    \n",
    "    noised_tokens = torch.where(bernoulli_mask.bool(), sampled_tokens, noised_tokens)\n",
    "    # Count corrupted tokens per sequence\n",
    "    #corrupted_count = (noised_tokens != tokens.unsqueeze(1)).sum(dim=-1)\n",
    "    \n",
    "    return noised_tokens\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def noise_operator_metadata(tokens, unigram_probabilities, rho: float, n_gram: int, num_samples: int):\n",
    "    batch_size, seq_len = tokens.shape\n",
    "    device = tokens.device\n",
    "    \n",
    "    mask = torch.ones((batch_size, seq_len), dtype=torch.bool, device=device)\n",
    "    mask[:, -n_gram:] = False\n",
    "    \n",
    "    noised_tokens = tokens.unsqueeze(1).expand(-1, num_samples, -1).clone()\n",
    "    \n",
    "    bernoulli_mask = torch.bernoulli(torch.full((batch_size, num_samples, seq_len), rho, device=device))\n",
    "    bernoulli_mask = bernoulli_mask * mask.unsqueeze(1)\n",
    "    \n",
    "    sampled_tokens = torch.multinomial(\n",
    "        unigram_probabilities,\n",
    "        batch_size * num_samples * seq_len,\n",
    "        replacement=True\n",
    "    ).view(batch_size, num_samples, seq_len)\n",
    "    \n",
    "    noised_tokens = torch.where(bernoulli_mask.bool(), sampled_tokens, noised_tokens)\n",
    "    \n",
    "    # Identify corrupted tokens\n",
    "    corrupted_mask = (noised_tokens != tokens.unsqueeze(1))\n",
    "    \n",
    "    # Count corrupted tokens per sequence\n",
    "    corrupted_count = corrupted_mask.sum(dim=-1)\n",
    "    \n",
    "    # Track positions of corrupted tokens\n",
    "    position_indices = torch.arange(seq_len, device=device).expand(batch_size, num_samples, -1)\n",
    "    corrupted_positions = position_indices[corrupted_mask]\n",
    "    \n",
    "    # Calculate distance from end for each corrupted token\n",
    "    distance_from_end = seq_len - corrupted_positions - 1\n",
    "    \n",
    "    return noised_tokens, corrupted_count, corrupted_positions, distance_from_end\n",
    "\n",
    "# Usage example:\n",
    "# noised_tokens, corrupted_count, corrupted_positions, distance_from_end = noise_operator(tokens, unigram_probabilities, rho, n_gram, num_samples)\n",
    "\n",
    "    \n",
    "def compare_noised_predictions_batched(model, batch, unigram_probabilities, rho, n_gram, num_samples, batch_size=10):\n",
    "    device = batch.device\n",
    "    orig_batch_size, seq_len = batch.shape\n",
    "\n",
    "    # Get original predictions and apply log_softmax\n",
    "    with torch.no_grad():\n",
    "        original_logits = log_softmax(model(batch)[:, -1:, :], dim=-1)\n",
    "\n",
    "    # Apply noise operator\n",
    "    noised_batches = noise_operator(\n",
    "        batch,\n",
    "        unigram_probabilities,\n",
    "        rho,\n",
    "        n_gram,\n",
    "        num_samples\n",
    "    )\n",
    "    \n",
    "    # Initialize tensor to store noised log_softmax probabilities\n",
    "    noised_log_probs = torch.zeros((orig_batch_size, num_samples, original_logits.shape[-1]), device='cpu')\n",
    "\n",
    "    # Process noised batches\n",
    "    for i in range(0, orig_batch_size):\n",
    "        end_i = min(i + 1, orig_batch_size)\n",
    "        for j in range(0, num_samples, batch_size):\n",
    "            end_j = min(j + batch_size, num_samples)\n",
    "            \n",
    "            current_batch = noised_batches[i:end_i, j:end_j].reshape(-1, seq_len)            \n",
    "            current_log_probs = run_minibatch(model, current_batch)\n",
    "            reshaped_log_probs = current_log_probs.view(end_i - i, end_j - j, -1)\n",
    "            noised_log_probs[i:end_i, j:end_j] = reshaped_log_probs.to('cpu')\n",
    "\n",
    "    return original_logits, noised_log_probs\n",
    "    \n",
    "\n",
    "def kl_divergence_log(p_log, q_log):\n",
    "    return (p_log.exp() * (p_log - q_log)).sum(dim=-1)\n",
    "\n",
    "\n",
    "def entropy(p_log):\n",
    "    return -1. * (softmax(p_log, dim=-1) * p_log).sum(dim=-1)\n",
    "\n",
    "\n",
    "def analyze_noise_effect(model, batch, unigram_probabilities, rho, n_gram, num_samples, batch_size=10):\n",
    "    original_log_probs, noised_log_probs = (\n",
    "        compare_noised_predictions_batched(model, batch, unigram_probabilities, rho, n_gram, num_samples, batch_size)\n",
    "    )\n",
    "\n",
    "    mean_noised_log_probs = log_softmax(noised_log_probs.mean(dim=1), dim=-1)\n",
    "\n",
    "    # Move tensors to CPU for calculations\n",
    "    original_log_probs = original_log_probs.cpu()\n",
    "    noised_log_probs = noised_log_probs.cpu()\n",
    "\n",
    "    # Calculate KL divergences in log space\n",
    "    kl_noised_original = kl_divergence_log(noised_log_probs, original_log_probs)\n",
    "    kl_mean_noised_original = kl_divergence_log(mean_noised_log_probs, original_log_probs.squeeze())\n",
    "\n",
    "    # Print some basic statistics for reference\n",
    "\n",
    "    avg_kl_noised_original = kl_noised_original.mean().item()\n",
    "    max_kl_noised_original =  kl_noised_original.max().item()\n",
    "    avg_kl_mean_noised_original = kl_mean_noised_original.mean().item()\n",
    "    max_kl_mean_noised_original =  kl_mean_noised_original.max().item()\n",
    "    \n",
    "\n",
    "    print(f\"Average KL(noised || original): {avg_kl_noised_original:.4f}\")\n",
    "    print(f\"Max KL(noised || original): {max_kl_noised_original:.4f}\")\n",
    "    print(f\"Average KL(mean noised || original): {avg_kl_mean_noised_original:.4f}\")\n",
    "    print(f\"Max KL(mean noised || original): {max_kl_mean_noised_original:.4f}\")\n",
    "\n",
    "    return original_log_probs.squeeze(), mean_noised_log_probs, kl_noised_original #, corrupted_positions, dist_from_end\n",
    "\n",
    "# Usage example\n",
    "# model = your_transformer_model\n",
    "# batch = your_input_batch\n",
    "# unigram_probabilities = your_unigram_probabilities\n",
    "# rho = 0.1\n",
    "# n_gram = 2\n",
    "# num_samples = 1000\n",
    "# batch_size = 10  # Adjust this based on your GPU memory\n",
    "# per_seq_std, euclidean_distance = analyze_noise_effect(model, batch, unigram_probabilities, rho, n_gram, num_samples, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6362da0c-47d9-4b3c-a5d6-b537b2834749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average KL(noised || original): 0.2717\n",
      "Max KL(noised || original): 9.3362\n",
      "Average KL(mean noised || original): 0.0358\n",
      "Max KL(mean noised || original): 0.1576\n"
     ]
    }
   ],
   "source": [
    "batch, _ = get_batch('train')\n",
    "log_probs, t_log_probs, kl_noised_original = analyze_noise_effect(tl_model, batch, unigrams, 0.1, 5, 256, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60d10e4a-54df-4c9c-b5ae-86420e33cb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.3794e-04, 3.0769e-02, 2.2824e-02, 4.5360e-01, 1.0706e+00, 1.4880e+00,\n",
       "        1.7449e+00, 4.5146e-03, 8.0832e-01, 3.0447e-01, 2.6097e-02, 2.4319e+00,\n",
       "        2.5486e+00, 1.6996e+00, 1.3079e-04, 1.2619e+00])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(t_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81b7d60c-7bec-4b2f-b2d3-5ef8ce691e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1989e-04, -2.5578e-02, -1.9680e-02,  3.8841e-02, -1.2229e-01,\n",
       "        -5.4508e-01, -3.0910e-01, -2.8948e-03, -2.7419e-01,  3.1388e-02,\n",
       "         7.1690e-03, -7.3609e-01, -4.2333e-01, -6.1854e-01, -3.1693e-05,\n",
       "        -4.4911e-01])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(log_probs.squeeze()) - entropy(t_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e5e7d5f-ffba-4242-940d-d369d158f3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average KL(noised || original): 0.2986\n",
      "Max KL(noised || original): 13.0858\n",
      "Average KL(mean noised || original): 0.0897\n",
      "Max KL(mean noised || original): 0.4492\n"
     ]
    }
   ],
   "source": [
    "log_probs, _, kl_noised_original = analyze_noise_effect(tl_model, batch, unigrams, 0.1, 5, 512, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d8438c1-e46c-4124-ab43-b39ff9ef6be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average KL(noised || original): 0.3129\n",
      "Max KL(noised || original): 12.2490\n",
      "Average KL(mean noised || original): 0.0929\n",
      "Max KL(mean noised || original): 0.4068\n"
     ]
    }
   ],
   "source": [
    "_, _, kl_noised_original = analyze_noise_effect(tl_model, batch, unigrams, 0.1, 5, 1024, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23d1046a-7ad7-4098-8b91-f601357f2ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0375e+00],\n",
       "        [7.5132e-04],\n",
       "        [7.6108e-03],\n",
       "        [1.4330e+00],\n",
       "        [1.6799e-01],\n",
       "        [1.4628e+00],\n",
       "        [1.6195e-03],\n",
       "        [5.4730e-01],\n",
       "        [6.8034e-01],\n",
       "        [6.7074e-02],\n",
       "        [1.9394e-02],\n",
       "        [1.2514e+00],\n",
       "        [1.2036e+00],\n",
       "        [1.5110e+00],\n",
       "        [1.8040e+00],\n",
       "        [1.9232e-01]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce6f4f-1d26-4bcc-b9f9-2a0ab6646a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_batch(batch_size=16, seq_len=1024):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "\n",
    "    data = np.memmap(data_dir / 'train.bin', dtype=np.uint16, mode='r')\n",
    "    \n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+seq_len]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x = x.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x = x.to(device)\n",
    "    return x\n",
    "\n",
    "\n",
    "def noise_sensitivity_for_checkpoint(model, num_sequences, unigram_probabilities, rho, n_gram, num_samples, sample_batch_size):\n",
    "    batch = get_batch('train')\n",
    "    \n",
    "\n",
    "\n",
    "def noise_sensitivity_over_time(epochs, num_sequences, unigram_probabilities, rho, n_gram, num_samples, sample_batch_size):\n",
    "    model_path = Path('/media/External01/out')\n",
    "    for step in epochs:\n",
    "        ckpt = torch.load(model_path / f'ckpt{epoch}.pt', map_location='cpu')\n",
    "        config = nanogpt_to_hooked_transformer_config(ckpt['model_args'])\n",
    "        tl_weights = convert_nanogpt_weights(ckpt['model'], config)\n",
    "        tl_model = HookedTransformer(config)\n",
    "        tl_model.load_state_dict(tl_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42978f-b523-442d-b44f-5296062827a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
