{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b33032-8cbd-4fa6-8ec2-1e59aff88f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from plotnine import (\n",
    "    ggplot,\n",
    "    geom_point, \n",
    "    geom_histogram, \n",
    "    geom_line,\n",
    "    geom_ribbon,\n",
    "    qplot, \n",
    "    coord_fixed, \n",
    "    aes, \n",
    "    facet_wrap, \n",
    "    labs,\n",
    "    scale_x_log10,\n",
    "    scale_y_log10\n",
    ")\n",
    "import polars as pl\n",
    "import torch\n",
    "\n",
    "from tokengrams import MemmapIndex, InMemoryIndex\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens import HookedTransformerConfig\n",
    "import zstandard as zstd\n",
    "\n",
    "\n",
    "from ngram_markov.utils import create_ngrams, nanogpt_to_hooked_transformer_config, convert_nanogpt_weights\n",
    "from ngram_markov.model import GPT, GPTConfig\n",
    "from ngram_markov.hooked_transformer import HookedTransformer\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import rustworkx as rx\n",
    "\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e049bbd-35f1-4dbc-a870-1c5e30155526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def noise_operator(tokens, unigram_probabilities, rho: float, n_gram: int, num_samples: int):\n",
    "    batch_size, seq_len = tokens.shape\n",
    "    device = tokens.device\n",
    "    \n",
    "    # Create a mask for the positions we'll potentially modify\n",
    "    mask = torch.ones((batch_size, seq_len), dtype=torch.bool, device=device)\n",
    "    mask[:, -n_gram:] = False\n",
    "    \n",
    "    # Expand tokens for num_samples\n",
    "    noised_tokens = tokens.unsqueeze(1).expand(-1, num_samples, -1).clone()\n",
    "    \n",
    "    # Create Bernoulli mask\n",
    "    bernoulli_mask = torch.bernoulli(torch.full((batch_size, num_samples, seq_len), rho, device=device))\n",
    "    bernoulli_mask = bernoulli_mask * mask.unsqueeze(1)\n",
    "    \n",
    "    # Sample from unigram distribution\n",
    "    sampled_tokens = torch.multinomial(\n",
    "        unigram_probabilities,\n",
    "        batch_size * num_samples * seq_len,\n",
    "        replacement=True\n",
    "    ).view(batch_size, num_samples, seq_len)\n",
    "    \n",
    "    # Apply noise\n",
    "    noised_tokens = torch.where(bernoulli_mask.bool(), sampled_tokens, noised_tokens)\n",
    "    \n",
    "    return noised_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab585ac-7674-4061-bf53-f045106ef89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 53_000\n",
    "model_path = Path('/media/External01/ngram-checkpoints/4layer_tinystories')\n",
    "\n",
    "#ckpt = torch.load(, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c77651-2563-4443-8f5f-0ca0ad26e6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ngram_markov.model import GPT, GPTConfig\n",
    "from ngram_markov.utils import create_ngrams, nanogpt_to_hooked_transformer_config, convert_nanogpt_weights\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "import einops\n",
    "import torch\n",
    "import plotly.express as px\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "\n",
    "\n",
    "def load_nnsight_model(path):\n",
    "    ckpt = torch.load(path, map_location='cpu')\n",
    "    config = nanogpt_to_hooked_transformer_config(ckpt['model_args'])\n",
    "    tl_weights = convert_nanogpt_weights(ckpt['model'], config)\n",
    "    tl_model = HookedTransformer(config)\n",
    "    tl_model.load_state_dict(tl_weights)\n",
    "    return  LanguageModel(tl_model)\n",
    "\n",
    "\n",
    "def load_tl_model(path):\n",
    "    ckpt = torch.load(path, map_location='cpu')\n",
    "    config = nanogpt_to_hooked_transformer_config(ckpt['model_args'])\n",
    "    tl_weights = convert_nanogpt_weights(ckpt['model'], config)\n",
    "    tl_model = HookedTransformer(config)\n",
    "    tl_model.load_state_dict(tl_weights)\n",
    "    return tl_model\n",
    "\n",
    "model = load_tl_model(model_path / f'ckpt{epoch}.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd13149-c4bc-4461-ac57-6a42731c6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram_markov.model import GPT, GPTConfig\n",
    "from ngram_markov.utils import create_ngrams, nanogpt_to_hooked_transformer_config, convert_nanogpt_weights\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#gpt_config = GPTConfig(**ckpt['model_args'])\n",
    "\n",
    "#gpt_model = GPT(gpt_config)\n",
    "#gpt_model.load_state_dict(ckpt['model'])\n",
    "#gpt_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ae91c2-1bde-4789-a988-3e8e42ca27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_local_attention_mask(seq_len: int, window_size: int, batch_size: int = 1, device='cpu') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates a mask for local attention where each token can only attend to\n",
    "    the previous window_size tokens and itself, while maintaining causality.\n",
    "    \n",
    "    Args:\n",
    "    seq_len (int): The sequence length.\n",
    "    window_size (int): The number of previous tokens to attend to.\n",
    "    batch_size (int): The batch size.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: A boolean mask of shape (batch_size, 1, seq_len, seq_len).\n",
    "    \"\"\"\n",
    "    # Create a causal mask\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    \n",
    "    # Create a local attention mask\n",
    "    local_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1-window_size)\n",
    "    \n",
    "    # Combine causal and local masks\n",
    "    mask = causal_mask * local_mask\n",
    "    \n",
    "    # Add batch and head dimensions\n",
    "    mask = mask.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, seq_len, seq_len)\n",
    "    mask = torch.logical_not(mask).float().masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    return mask.to(device)\n",
    "\n",
    "# Example usage:\n",
    "# seq_len = 1024\n",
    "# window_size = 256\n",
    "# batch_size = 1\n",
    "# local_mask = create_local_attention_mask(seq_len, window_size, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2456b9c2-0cf9-415b-8e81-5e5141a88577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [-inf, 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "          [-inf, -inf, 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "          [-inf, -inf, -inf, 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "          [-inf, -inf, -inf, -inf, 0., 0., 0., -inf, -inf, -inf],\n",
       "          [-inf, -inf, -inf, -inf, -inf, 0., 0., 0., -inf, -inf],\n",
       "          [-inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., -inf],\n",
       "          [-inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_local_attention_mask(10, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "821be44c-2dab-4d35-b196-2e77cd568166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram_markov.ngrams import create_ngrams, calculate_ngram_kl_divergence\n",
    "\n",
    "\n",
    "data_dir = Path('data/tinystories')\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 256\n",
    "device_type = 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(data_dir / 'train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(data_dir / 'validation.bin', dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x\n",
    "    else:\n",
    "        x = x.to(device)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30fe708c-f04b-461f-bf0c-5b9e45365811",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfe73ac4-284b-441d-a6da-892c35cc30c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3166, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_mask = create_local_attention_mask(block_size, 2, 16)\n",
    "full_preds = model(batch)\n",
    "local_preds = model(batch, additive_attention_mask=local_mask.to('cuda:0'))\n",
    "(full_preds - local_preds).abs().mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a60de6d-b3ba-4605-bfe6-680243ef77fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256, 4, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, cache = model.run_with_cache(batch)\n",
    "cache['blocks.0.attn.hook_z'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7316b80-7382-4a4b-871c-26bcd5fe9e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a36809abb604ee4b9b237fe91c95c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from ngram_markov.ngrams import kl_divergence\n",
    "\n",
    "def run_model_and_cache(model, input_ids, window_size=5, use_local_attention=False):\n",
    "    if use_local_attention:\n",
    "        additive_mask = create_local_attention_mask(input_ids.shape[1], window_size=window_size, batch_size=input_ids.shape[0], device='cuda:0')\n",
    "        output, cache = model.run_with_cache(input_ids, additive_attention_mask=additive_mask)\n",
    "    else:\n",
    "        output, cache = model.run_with_cache(input_ids)\n",
    "    return output.to('cpu'), cache.to('cpu')\n",
    "\n",
    "def patch_head_output(model, input_ids, source_cache, target_cache, layer, head, use_local_attention, window_size=2):\n",
    "    hook_point = f'blocks.{layer}.attn.hook_z'\n",
    "    def patch_hook(value, hook):\n",
    "        patched_value = value.clone()\n",
    "        #print((patched_value[:, :, head, :].to('cpu') - source_cache[hook_point][:, :, head, :]).abs().max())\n",
    "        patched_value[:, :, head, :] = source_cache[hook_point][:, :, head, :].to('cuda:0')\n",
    "        \n",
    "        return patched_value\n",
    "    \n",
    "    if use_local_attention:\n",
    "        additive_mask = create_local_attention_mask(input_ids.shape[1], window_size=window_size, batch_size=input_ids.shape[0], device='cuda:0')\n",
    "        patched_output = model.run_with_hooks(input_ids, additive_attention_mask=additive_mask, fwd_hooks=[(hook_point, patch_hook)])\n",
    "    else:\n",
    "        patched_output = model.run_with_hooks(input_ids, fwd_hooks=[(hook_point, patch_hook)])\n",
    "    return patched_output\n",
    "\n",
    "\n",
    "\n",
    "def compute_impact(patched_output, baseline_output, window_size):\n",
    "    #print((patched_output.to('cpu') - baseline_output).abs().max())\n",
    "    return kl_divergence(\n",
    "        patched_output.log_softmax(dim=-1).reshape(-1, 512)[window_size:, :],\n",
    "        baseline_output.log_softmax(dim=-1).to('cuda:0').reshape(-1, 512)[window_size:, :],\n",
    "    )\n",
    "\n",
    "def analyze_attention_head_differences(model, input_ids, window_size=2):\n",
    "    full_output, full_cache = run_model_and_cache(model, input_ids)\n",
    "    local_output, local_cache = run_model_and_cache(model, input_ids, use_local_attention=True, window_size=window_size)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for layer in range(model.cfg.n_layers):        \n",
    "        for head in range(model.cfg.n_heads):\n",
    "            record = {'layer': layer, 'head': head}\n",
    "            # Patch full into local\n",
    "            patched_output_full_to_local = patch_head_output(model, input_ids, full_cache, local_cache, layer, head, use_local_attention=True, window_size=window_size)\n",
    "            record['f2l_vs_local'] =  compute_impact(patched_output_full_to_local, local_output, window_size).cpu().numpy()\n",
    "            record['f2l_vs_full'] = compute_impact(patched_output_full_to_local, full_output, window_size).cpu().numpy()\n",
    "            \n",
    "            # Patch local into full\n",
    "            patched_output_local_to_full = patch_head_output(model, input_ids, local_cache, full_cache, layer, head, use_local_attention=False, window_size=window_size)\n",
    "            \n",
    "            record['l2f_vs_full'] = compute_impact(patched_output_local_to_full, full_output, window_size).cpu().numpy()\n",
    "            record['l2f_vs_local'] = compute_impact(patched_output_local_to_full, local_output, window_size).cpu().numpy()\n",
    "            df = pl.DataFrame(record).with_columns(head=pl.lit(head), layer=pl.lit(layer))\n",
    "            results.append(df)\n",
    "    \n",
    "    return pl.concat(results, how='vertical')\n",
    "\n",
    "# Usage\n",
    "input_ids = get_batch('train')\n",
    "model.to('cuda:0')\n",
    "all_diffs = []\n",
    "num_batches = 1000\n",
    "for _ in trange(num_batches):\n",
    "    input_ids = get_batch('train')\n",
    "    with torch.no_grad():\n",
    "        all_diffs.append(analyze_attention_head_differences(model, input_ids.to('cuda'), window_size=5))\n",
    "patching_df = pl.concat(all_diffs, how='vertical').melt(id_vars=['layer', 'head'])\n",
    "patching_df.write_parquet('patching_results_window_5.parquet')\n",
    "# Analyze results\n",
    "#for direction in ['full_to_local', 'local_to_full']:\n",
    "#    print(f\"\\nTop 10 most impactful heads ({direction}):\")\n",
    "#    top_differences = sorted(differences[direction].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "#    for head, impact in top_differences:\n",
    "#        print(f\"{head}: {impact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d20defec-6421-4e30-a494-a87b632fb91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>head</th><th>min_diff</th><th>mean_diff</th><th>med_diff</th><th>max_diff</th><th>q25_diff</th><th>q75_diff</th><th>q90_diff</th><th>q95_diff</th></tr><tr><td>i32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>-6.8308e-7</td><td>0.799904</td><td>0.521741</td><td>17.296215</td><td>0.251066</td><td>1.045621</td><td>1.847112</td><td>2.505945</td></tr><tr><td>1</td><td>-6.8308e-7</td><td>1.083738</td><td>0.801561</td><td>20.4261</td><td>0.391188</td><td>1.470269</td><td>2.367022</td><td>3.063566</td></tr><tr><td>2</td><td>-6.8308e-7</td><td>0.802297</td><td>0.629361</td><td>15.186031</td><td>0.356174</td><td>1.04748</td><td>1.620571</td><td>2.088233</td></tr><tr><td>3</td><td>-6.8308e-7</td><td>0.746013</td><td>0.583443</td><td>13.601524</td><td>0.331211</td><td>0.975181</td><td>1.514431</td><td>1.952505</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 9)\n",
       "┌──────┬────────────┬───────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       "│ head ┆ min_diff   ┆ mean_diff ┆ med_diff ┆ … ┆ q25_diff ┆ q75_diff ┆ q90_diff ┆ q95_diff │\n",
       "│ ---  ┆ ---        ┆ ---       ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       "│ i32  ┆ f32        ┆ f32       ┆ f32      ┆   ┆ f32      ┆ f32      ┆ f32      ┆ f32      │\n",
       "╞══════╪════════════╪═══════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       "│ 0    ┆ -6.8308e-7 ┆ 0.799904  ┆ 0.521741 ┆ … ┆ 0.251066 ┆ 1.045621 ┆ 1.847112 ┆ 2.505945 │\n",
       "│ 1    ┆ -6.8308e-7 ┆ 1.083738  ┆ 0.801561 ┆ … ┆ 0.391188 ┆ 1.470269 ┆ 2.367022 ┆ 3.063566 │\n",
       "│ 2    ┆ -6.8308e-7 ┆ 0.802297  ┆ 0.629361 ┆ … ┆ 0.356174 ┆ 1.04748  ┆ 1.620571 ┆ 2.088233 │\n",
       "│ 3    ┆ -6.8308e-7 ┆ 0.746013  ┆ 0.583443 ┆ … ┆ 0.331211 ┆ 0.975181 ┆ 1.514431 ┆ 1.952505 │\n",
       "└──────┴────────────┴───────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    patching_df\n",
    "    .filter(pl.col('layer').eq(1) & pl.col('variable').eq('f2l_vs_local'))\n",
    "    .group_by('head')\n",
    "    .agg(\n",
    "        min_diff = pl.col('value').min(),\n",
    "        mean_diff = pl.col('value').mean(),\n",
    "        med_diff = pl.col('value').median(),\n",
    "        max_diff = pl.col('value').max(),\n",
    "        q25_diff = pl.col('value').quantile(0.25),\n",
    "        q75_diff = pl.col('value').quantile(0.75), \n",
    "        q90_diff = pl.col('value').quantile(0.90),\n",
    "        q95_diff = pl.col('value').quantile(0.95),\n",
    "    )\n",
    "    .sort('head')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2597fe2d-9e1d-44da-8de3-1cd2f1797f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (680, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>layer</th><th>head</th><th>variable</th><th>value</th></tr><tr><td>i32</td><td>i32</td><td>str</td><td>f32</td></tr></thead><tbody><tr><td>1</td><td>1</td><td>&quot;f2l_vs_local&quot;</td><td>20.4261</td></tr><tr><td>1</td><td>1</td><td>&quot;f2l_vs_local&quot;</td><td>16.592453</td></tr><tr><td>1</td><td>1</td><td>&quot;f2l_vs_local&quot;</td><td>15.332425</td></tr><tr><td>1</td><td>1</td><td>&quot;f2l_vs_local&quot;</td><td>15.306915</td></tr><tr><td>1</td><td>1</td><td>&quot;f2l_vs_local&quot;</td><td>14.842405</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1</td><td>1</td><td>&quot;f2l_vs_local&quot;</td><td>10.007487</td></tr><tr><td>1</td><td>1</td><td>&quot;f2l_vs_local&quot;</td><td>10.006559</td></tr><tr><td>1</td><td>1</td><td>&quot;f2l_vs_local&quot;</td><td>10.005215</td></tr><tr><td>1</td><td>1</td><td>&quot;f2l_vs_local&quot;</td><td>10.004627</td></tr><tr><td>1</td><td>1</td><td>&quot;f2l_vs_local&quot;</td><td>10.002022</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (680, 4)\n",
       "┌───────┬──────┬──────────────┬───────────┐\n",
       "│ layer ┆ head ┆ variable     ┆ value     │\n",
       "│ ---   ┆ ---  ┆ ---          ┆ ---       │\n",
       "│ i32   ┆ i32  ┆ str          ┆ f32       │\n",
       "╞═══════╪══════╪══════════════╪═══════════╡\n",
       "│ 1     ┆ 1    ┆ f2l_vs_local ┆ 20.4261   │\n",
       "│ 1     ┆ 1    ┆ f2l_vs_local ┆ 16.592453 │\n",
       "│ 1     ┆ 1    ┆ f2l_vs_local ┆ 15.332425 │\n",
       "│ 1     ┆ 1    ┆ f2l_vs_local ┆ 15.306915 │\n",
       "│ 1     ┆ 1    ┆ f2l_vs_local ┆ 14.842405 │\n",
       "│ …     ┆ …    ┆ …            ┆ …         │\n",
       "│ 1     ┆ 1    ┆ f2l_vs_local ┆ 10.007487 │\n",
       "│ 1     ┆ 1    ┆ f2l_vs_local ┆ 10.006559 │\n",
       "│ 1     ┆ 1    ┆ f2l_vs_local ┆ 10.005215 │\n",
       "│ 1     ┆ 1    ┆ f2l_vs_local ┆ 10.004627 │\n",
       "│ 1     ┆ 1    ┆ f2l_vs_local ┆ 10.002022 │\n",
       "└───────┴──────┴──────────────┴───────────┘"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    patching_df\n",
    "    .filter(\n",
    "        pl.col('layer').eq(1) & \n",
    "        pl.col('variable').eq('f2l_vs_local') &\n",
    "        pl.col('head').eq(1) &\n",
    "        pl.col('value').gt(10)\n",
    "        \n",
    "    )\n",
    "    .sort('value', descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fcacdd92-0e6d-4acd-a960-30de01b11354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>head</th><th>min_diff</th><th>mean_diff</th><th>med_diff</th><th>max_diff</th><th>q25_diff</th><th>q75_diff</th><th>q90_diff</th><th>q95_diff</th></tr><tr><td>i32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>-6.8308e-7</td><td>0.624556</td><td>0.072269</td><td>33.463722</td><td>0.00974</td><td>0.279776</td><td>1.090482</td><td>3.269177</td></tr><tr><td>1</td><td>-6.8308e-7</td><td>1.151524</td><td>0.257083</td><td>29.727833</td><td>0.045249</td><td>1.000877</td><td>3.357298</td><td>6.25361</td></tr><tr><td>2</td><td>-6.8308e-7</td><td>0.399355</td><td>0.103724</td><td>28.012333</td><td>0.011397</td><td>0.342546</td><td>0.907622</td><td>1.692867</td></tr><tr><td>3</td><td>-6.8308e-7</td><td>0.276543</td><td>0.086751</td><td>18.841259</td><td>0.004821</td><td>0.300218</td><td>0.7069</td><td>1.156817</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 9)\n",
       "┌──────┬────────────┬───────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       "│ head ┆ min_diff   ┆ mean_diff ┆ med_diff ┆ … ┆ q25_diff ┆ q75_diff ┆ q90_diff ┆ q95_diff │\n",
       "│ ---  ┆ ---        ┆ ---       ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       "│ i32  ┆ f32        ┆ f32       ┆ f32      ┆   ┆ f32      ┆ f32      ┆ f32      ┆ f32      │\n",
       "╞══════╪════════════╪═══════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       "│ 0    ┆ -6.8308e-7 ┆ 0.624556  ┆ 0.072269 ┆ … ┆ 0.00974  ┆ 0.279776 ┆ 1.090482 ┆ 3.269177 │\n",
       "│ 1    ┆ -6.8308e-7 ┆ 1.151524  ┆ 0.257083 ┆ … ┆ 0.045249 ┆ 1.000877 ┆ 3.357298 ┆ 6.25361  │\n",
       "│ 2    ┆ -6.8308e-7 ┆ 0.399355  ┆ 0.103724 ┆ … ┆ 0.011397 ┆ 0.342546 ┆ 0.907622 ┆ 1.692867 │\n",
       "│ 3    ┆ -6.8308e-7 ┆ 0.276543  ┆ 0.086751 ┆ … ┆ 0.004821 ┆ 0.300218 ┆ 0.7069   ┆ 1.156817 │\n",
       "└──────┴────────────┴───────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    patching_df\n",
    "    .filter(pl.col('layer').eq(1) & pl.col('variable').eq('l2f_vs_full'))\n",
    "    .group_by('head')\n",
    "    .agg(\n",
    "        min_diff = pl.col('value').min(),\n",
    "        mean_diff = pl.col('value').mean(),\n",
    "        med_diff = pl.col('value').median(),\n",
    "        max_diff = pl.col('value').max(),\n",
    "        q25_diff = pl.col('value').quantile(0.25),\n",
    "        q75_diff = pl.col('value').quantile(0.75), \n",
    "        q90_diff = pl.col('value').quantile(0.90),\n",
    "        q95_diff = pl.col('value').quantile(0.95),\n",
    "    )\n",
    "    .sort('head')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c874b8a4-41bd-4f6f-a94c-be4e37b30124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>layer</th><th>head</th><th>variable</th><th>mean_kl</th></tr><tr><td>i32</td><td>i32</td><td>str</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>3</td><td>&quot;f2l_vs_full&quot;</td><td>7.550286</td></tr><tr><td>0</td><td>3</td><td>&quot;f2l_vs_local&quot;</td><td>1.474907</td></tr><tr><td>0</td><td>3</td><td>&quot;l2f_vs_full&quot;</td><td>4.649981</td></tr><tr><td>0</td><td>3</td><td>&quot;l2f_vs_local&quot;</td><td>2.999097</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 4)\n",
       "┌───────┬──────┬──────────────┬──────────┐\n",
       "│ layer ┆ head ┆ variable     ┆ mean_kl  │\n",
       "│ ---   ┆ ---  ┆ ---          ┆ ---      │\n",
       "│ i32   ┆ i32  ┆ str          ┆ f32      │\n",
       "╞═══════╪══════╪══════════════╪══════════╡\n",
       "│ 0     ┆ 3    ┆ f2l_vs_full  ┆ 7.550286 │\n",
       "│ 0     ┆ 3    ┆ f2l_vs_local ┆ 1.474907 │\n",
       "│ 0     ┆ 3    ┆ l2f_vs_full  ┆ 4.649981 │\n",
       "│ 0     ┆ 3    ┆ l2f_vs_local ┆ 2.999097 │\n",
       "└───────┴──────┴──────────────┴──────────┘"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(pl.col('layer').eq(0) & pl.col('head').eq(3)).sort('variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1653a666-12be-40e7-ad7c-381fbf623ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [[i] for i in range(512)]\n",
    "bigram_counts = torch.tensor(index.batch_count_next(queries, 512), dtype=torch.float32)\n",
    "unigrams = bigram_counts.sum(dim=1)\n",
    "unigrams /= unigrams.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f48c7184-4516-470a-8575-8e526088d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, _ = get_batch('train')\n",
    "\n",
    "\n",
    "\n",
    "noised_batch = noise_operator(batch, unigrams, 0.1, 4, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0a73f5e-c6db-4a1e-a90f-b9f7f5fa9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_minibatch(model, minibatch):\n",
    "    with torch.amp.autocast('cuda'):\n",
    "        logits = model(minibatch.to('cuda'))[:, -1:, :]\n",
    "        current_log_probs = log_softmax(logits, dim=-1)\n",
    "    return current_log_probs.to('cpu')\n",
    "    \n",
    "\n",
    "def noise_operator(tokens, unigram_probabilities, rho: float, n_gram: int, num_samples: int):\n",
    "    batch_size, seq_len = tokens.shape\n",
    "    device = tokens.device\n",
    "    \n",
    "    mask = torch.ones((batch_size, seq_len), dtype=torch.bool, device=device)\n",
    "    mask[:, -n_gram:] = False\n",
    "    \n",
    "    noised_tokens = tokens.unsqueeze(1).expand(-1, num_samples, -1).clone()\n",
    "    \n",
    "    bernoulli_mask = torch.bernoulli(torch.full((batch_size, num_samples, seq_len), rho, device=device))\n",
    "    bernoulli_mask = bernoulli_mask * mask.unsqueeze(1)\n",
    "    \n",
    "    sampled_tokens = torch.multinomial(\n",
    "        unigram_probabilities,\n",
    "        batch_size * num_samples * seq_len,\n",
    "        replacement=True\n",
    "    ).view(batch_size, num_samples, seq_len)\n",
    "    \n",
    "    noised_tokens = torch.where(bernoulli_mask.bool(), sampled_tokens, noised_tokens)\n",
    "    # Count corrupted tokens per sequence\n",
    "    #corrupted_count = (noised_tokens != tokens.unsqueeze(1)).sum(dim=-1)\n",
    "    \n",
    "    return noised_tokens\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def noise_operator_metadata(tokens, unigram_probabilities, rho: float, n_gram: int, num_samples: int):\n",
    "    batch_size, seq_len = tokens.shape\n",
    "    device = tokens.device\n",
    "    \n",
    "    mask = torch.ones((batch_size, seq_len), dtype=torch.bool, device=device)\n",
    "    mask[:, -n_gram:] = False\n",
    "    \n",
    "    noised_tokens = tokens.unsqueeze(1).expand(-1, num_samples, -1).clone()\n",
    "    \n",
    "    bernoulli_mask = torch.bernoulli(torch.full((batch_size, num_samples, seq_len), rho, device=device))\n",
    "    bernoulli_mask = bernoulli_mask * mask.unsqueeze(1)\n",
    "    \n",
    "    sampled_tokens = torch.multinomial(\n",
    "        unigram_probabilities,\n",
    "        batch_size * num_samples * seq_len,\n",
    "        replacement=True\n",
    "    ).view(batch_size, num_samples, seq_len)\n",
    "    \n",
    "    noised_tokens = torch.where(bernoulli_mask.bool(), sampled_tokens, noised_tokens)\n",
    "    \n",
    "    # Identify corrupted tokens\n",
    "    corrupted_mask = (noised_tokens != tokens.unsqueeze(1))\n",
    "    \n",
    "    # Count corrupted tokens per sequence\n",
    "    corrupted_count = corrupted_mask.sum(dim=-1)\n",
    "    \n",
    "    # Track positions of corrupted tokens\n",
    "    position_indices = torch.arange(seq_len, device=device).expand(batch_size, num_samples, -1)\n",
    "    corrupted_positions = position_indices[corrupted_mask]\n",
    "    \n",
    "    # Calculate distance from end for each corrupted token\n",
    "    distance_from_end = seq_len - corrupted_positions - 1\n",
    "    \n",
    "    return noised_tokens, corrupted_count, corrupted_positions, distance_from_end\n",
    "\n",
    "# Usage example:\n",
    "# noised_tokens, corrupted_count, corrupted_positions, distance_from_end = noise_operator(tokens, unigram_probabilities, rho, n_gram, num_samples)\n",
    "\n",
    "    \n",
    "def compare_noised_predictions_batched(model, batch, unigram_probabilities, rho, n_gram, num_samples, batch_size=10):\n",
    "    device = batch.device\n",
    "    orig_batch_size, seq_len = batch.shape\n",
    "\n",
    "    # Get original predictions and apply log_softmax\n",
    "    with torch.no_grad():\n",
    "        original_logits = log_softmax(model(batch)[:, -1:, :], dim=-1)\n",
    "\n",
    "    # Apply noise operator\n",
    "    noised_batches = noise_operator(\n",
    "        batch,\n",
    "        unigram_probabilities,\n",
    "        rho,\n",
    "        n_gram,\n",
    "        num_samples\n",
    "    )\n",
    "    \n",
    "    # Initialize tensor to store noised log_softmax probabilities\n",
    "    noised_log_probs = torch.zeros((orig_batch_size, num_samples, original_logits.shape[-1]), device='cpu')\n",
    "\n",
    "    # Process noised batches\n",
    "    for i in range(0, orig_batch_size):\n",
    "        end_i = min(i + 1, orig_batch_size)\n",
    "        for j in range(0, num_samples, batch_size):\n",
    "            end_j = min(j + batch_size, num_samples)\n",
    "            \n",
    "            current_batch = noised_batches[i:end_i, j:end_j].reshape(-1, seq_len)            \n",
    "            current_log_probs = run_minibatch(model, current_batch)\n",
    "            reshaped_log_probs = current_log_probs.view(end_i - i, end_j - j, -1)\n",
    "            noised_log_probs[i:end_i, j:end_j] = reshaped_log_probs.to('cpu')\n",
    "\n",
    "    return original_logits, noised_log_probs\n",
    "    \n",
    "\n",
    "def kl_divergence_log(p_log, q_log):\n",
    "    return (p_log.exp() * (p_log - q_log)).sum(dim=-1)\n",
    "\n",
    "\n",
    "def entropy(p_log):\n",
    "    return -1. * (softmax(p_log, dim=-1) * p_log).sum(dim=-1)\n",
    "\n",
    "\n",
    "def analyze_noise_effect(model, batch, unigram_probabilities, rho, n_gram, num_samples, batch_size=10):\n",
    "    original_log_probs, noised_log_probs = (\n",
    "        compare_noised_predictions_batched(model, batch, unigram_probabilities, rho, n_gram, num_samples, batch_size)\n",
    "    )\n",
    "\n",
    "    mean_noised_log_probs = log_softmax(noised_log_probs.mean(dim=1), dim=-1)\n",
    "\n",
    "    # Move tensors to CPU for calculations\n",
    "    original_log_probs = original_log_probs.cpu()\n",
    "    noised_log_probs = noised_log_probs.cpu()\n",
    "\n",
    "    # Calculate KL divergences in log space\n",
    "    kl_noised_original = kl_divergence_log(noised_log_probs, original_log_probs)\n",
    "    kl_mean_noised_original = kl_divergence_log(mean_noised_log_probs, original_log_probs.squeeze())\n",
    "\n",
    "    # Print some basic statistics for reference\n",
    "\n",
    "    avg_kl_noised_original = kl_noised_original.mean().item()\n",
    "    max_kl_noised_original =  kl_noised_original.max().item()\n",
    "    avg_kl_mean_noised_original = kl_mean_noised_original.mean().item()\n",
    "    max_kl_mean_noised_original =  kl_mean_noised_original.max().item()\n",
    "    \n",
    "\n",
    "    print(f\"Average KL(noised || original): {avg_kl_noised_original:.4f}\")\n",
    "    print(f\"Max KL(noised || original): {max_kl_noised_original:.4f}\")\n",
    "    print(f\"Average KL(mean noised || original): {avg_kl_mean_noised_original:.4f}\")\n",
    "    print(f\"Max KL(mean noised || original): {max_kl_mean_noised_original:.4f}\")\n",
    "\n",
    "    return original_log_probs.squeeze(), mean_noised_log_probs, kl_noised_original #, corrupted_positions, dist_from_end\n",
    "\n",
    "# Usage example\n",
    "# model = your_transformer_model\n",
    "# batch = your_input_batch\n",
    "# unigram_probabilities = your_unigram_probabilities\n",
    "# rho = 0.1\n",
    "# n_gram = 2\n",
    "# num_samples = 1000\n",
    "# batch_size = 10  # Adjust this based on your GPU memory\n",
    "# per_seq_std, euclidean_distance = analyze_noise_effect(model, batch, unigram_probabilities, rho, n_gram, num_samples, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6362da0c-47d9-4b3c-a5d6-b537b2834749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average KL(noised || original): 0.2717\n",
      "Max KL(noised || original): 9.3362\n",
      "Average KL(mean noised || original): 0.0358\n",
      "Max KL(mean noised || original): 0.1576\n"
     ]
    }
   ],
   "source": [
    "batch, _ = get_batch('train')\n",
    "log_probs, t_log_probs, kl_noised_original = analyze_noise_effect(tl_model, batch, unigrams, 0.1, 5, 256, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60d10e4a-54df-4c9c-b5ae-86420e33cb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.3794e-04, 3.0769e-02, 2.2824e-02, 4.5360e-01, 1.0706e+00, 1.4880e+00,\n",
       "        1.7449e+00, 4.5146e-03, 8.0832e-01, 3.0447e-01, 2.6097e-02, 2.4319e+00,\n",
       "        2.5486e+00, 1.6996e+00, 1.3079e-04, 1.2619e+00])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(t_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81b7d60c-7bec-4b2f-b2d3-5ef8ce691e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1989e-04, -2.5578e-02, -1.9680e-02,  3.8841e-02, -1.2229e-01,\n",
       "        -5.4508e-01, -3.0910e-01, -2.8948e-03, -2.7419e-01,  3.1388e-02,\n",
       "         7.1690e-03, -7.3609e-01, -4.2333e-01, -6.1854e-01, -3.1693e-05,\n",
       "        -4.4911e-01])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(log_probs.squeeze()) - entropy(t_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e5e7d5f-ffba-4242-940d-d369d158f3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average KL(noised || original): 0.2986\n",
      "Max KL(noised || original): 13.0858\n",
      "Average KL(mean noised || original): 0.0897\n",
      "Max KL(mean noised || original): 0.4492\n"
     ]
    }
   ],
   "source": [
    "log_probs, _, kl_noised_original = analyze_noise_effect(tl_model, batch, unigrams, 0.1, 5, 512, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd5c32-10f1-4035-b36c-e168cf77ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer/tinystories512\")\n",
    "ts_bin_path = 'data/tinystories/train.bin'\n",
    "index_path = \"data/tinystories/ngrams/suffix_tree.idx\"\n",
    "\n",
    "index = MemmapIndex(ts_bin_path, index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d8438c1-e46c-4124-ab43-b39ff9ef6be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average KL(noised || original): 0.3129\n",
      "Max KL(noised || original): 12.2490\n",
      "Average KL(mean noised || original): 0.0929\n",
      "Max KL(mean noised || original): 0.4068\n"
     ]
    }
   ],
   "source": [
    "_, _, kl_noised_original = analyze_noise_effect(tl_model, batch, unigrams, 0.1, 5, 1024, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23d1046a-7ad7-4098-8b91-f601357f2ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0375e+00],\n",
       "        [7.5132e-04],\n",
       "        [7.6108e-03],\n",
       "        [1.4330e+00],\n",
       "        [1.6799e-01],\n",
       "        [1.4628e+00],\n",
       "        [1.6195e-03],\n",
       "        [5.4730e-01],\n",
       "        [6.8034e-01],\n",
       "        [6.7074e-02],\n",
       "        [1.9394e-02],\n",
       "        [1.2514e+00],\n",
       "        [1.2036e+00],\n",
       "        [1.5110e+00],\n",
       "        [1.8040e+00],\n",
       "        [1.9232e-01]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce6f4f-1d26-4bcc-b9f9-2a0ab6646a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_batch(batch_size=16, seq_len=1024):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "\n",
    "    data = np.memmap(data_dir / 'train.bin', dtype=np.uint16, mode='r')\n",
    "    \n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+seq_len]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x = x.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x = x.to(device)\n",
    "    return x\n",
    "\n",
    "\n",
    "def noise_sensitivity_for_checkpoint(model, num_sequences, unigram_probabilities, rho, n_gram, num_samples, sample_batch_size):\n",
    "    batch = get_batch('train')\n",
    "    \n",
    "\n",
    "\n",
    "def noise_sensitivity_over_time(epochs, num_sequences, unigram_probabilities, rho, n_gram, num_samples, sample_batch_size):\n",
    "    model_path = Path('/media/External01/out')\n",
    "    for step in epochs:\n",
    "        ckpt = torch.load(model_path / f'ckpt{epoch}.pt', map_location='cpu')\n",
    "        config = nanogpt_to_hooked_transformer_config(ckpt['model_args'])\n",
    "        tl_weights = convert_nanogpt_weights(ckpt['model'], config)\n",
    "        tl_model = HookedTransformer(config)\n",
    "        tl_model.load_state_dict(tl_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42978f-b523-442d-b44f-5296062827a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
