{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c820500c-7e8b-4f6b-93f8-704c66ed9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from plotnine import (\n",
    "    ggplot,\n",
    "    geom_point, \n",
    "    geom_histogram, \n",
    "    geom_line,\n",
    "    geom_ribbon,\n",
    "    qplot, \n",
    "    coord_fixed, \n",
    "    aes, \n",
    "    facet_wrap, \n",
    "    labs,\n",
    "    scale_x_log10,\n",
    "    scale_y_log10\n",
    ")\n",
    "import polars as pl\n",
    "import torch\n",
    "from scipy.sparse import coo_array, csr_array\n",
    "from scipy import linalg\n",
    "import scipy.sparse.linalg as sparselinalg\n",
    "from tokengrams import MemmapIndex, InMemoryIndex\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from ngram_markov.hooked_transformer import HookedTransformer\n",
    "from transformer_lens import HookedTransformerConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e8a4e8-4d77-41e0-8633-9ba1ba97c57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82dfde5e-71ae-4cf8-aa52-5fb9e9d31ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ngram_markov.model import GPT, GPTConfig\n",
    "from ngram_markov.utils import create_ngrams, nanogpt_to_hooked_transformer_config, convert_nanogpt_weights\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "import einops\n",
    "import torch\n",
    "import plotly.express as px\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "\n",
    "def load_nnsight_model(path):\n",
    "    ckpt = torch.load(path, map_location='cpu')\n",
    "    config = nanogpt_to_hooked_transformer_config(ckpt['model_args'])\n",
    "    tl_weights = convert_nanogpt_weights(ckpt['model'], config)\n",
    "    tl_model = HookedTransformer(config)\n",
    "    tl_model.load_state_dict(tl_weights)\n",
    "    return  LanguageModel(tl_model)\n",
    "\n",
    "\n",
    "def load_tl_model(path):\n",
    "    ckpt = torch.load(path, map_location='cpu')\n",
    "    config = nanogpt_to_hooked_transformer_config(ckpt['model_args'])\n",
    "    tl_weights = convert_nanogpt_weights(ckpt['model'], config)\n",
    "    tl_model = HookedTransformer(config)\n",
    "    tl_model.load_state_dict(tl_weights)\n",
    "    return tl_model\n",
    "\n",
    "model = load_tl_model(model_path / f'ckpt{epoch}.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd73c5-7c29-4a94-ae8c-cd5e35cf917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac718c30-b098-4891-8076-87b11ea076ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data/tinystories')\n",
    "\n",
    "batch_size = 2048\n",
    "device_type = 'cuda'\n",
    "device = 'cuda'\n",
    "\n",
    "def get_batch(block_size=512):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    data = np.memmap(data_dir / 'train.bin', dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x = x.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x  = x.to(device)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad7d2cf4-c860-48ef-9670-84be181a8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68dbaa42-9c66-4009-be2e-d0c75288991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinystories = np.memmap(data_dir / 'train.bin', dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970db734-6386-44cf-84b7-4ab2d30b9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ngram_occurrences(index, raw_data, ngrams, min_occur, ctx_len):\n",
    "    n = ngrams.shape[1]\n",
    "    ngram_positions = [\n",
    "        index.positions(query)[:min_occur] for query in ngrams.cpu().tolist()\n",
    "    ]\n",
    "    mask = torch.tensor([len(pos) >= min_occur for pos in ngram_positions])\n",
    "    ngram_occurrences = []\n",
    "    for positions in ngram_positions:\n",
    "        if len(positions) < min_occur:\n",
    "            continue\n",
    "        occurrences = torch.stack([\n",
    "            torch.from_numpy(raw_data[(i-ctx_len):(i + n)].astype(np.int64)) for i in positions\n",
    "        ])\n",
    "        ngram_occurrences.append(occurrences.unsqueeze(0))\n",
    "    return torch.concat(ngram_occurrences), ngrams[mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb00efd-64e9-4ace-8a40-06fd84799e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_occurrences, reduced_ngrams = get_ngram_occurrences(index, tinystories, ngrams, 32, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b407d24-c503-486a-9f59-16c49456d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    base_pred = tl_model(reduced_ngrams)[:, -1, :].squeeze().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e3865-2aa0-48f7-a180-51e658cb9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in ngram_occurrences.reshape(-1, 8).split(128):\n",
    "        model_preds.append(tl_model(batch.to('cuda'))[:, -1, :].to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6975ce-978f-4c1c-8788-fba514f03eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.concat(model_preds)\n",
    "preds.reshape(32, -1, 512).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0110b-1a90-47fc-90ff-8335367cb4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c9c4bbc-d854-40ed-908a-82e8a9855ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data/tinystories')\n",
    "\n",
    "batch_size = 16_384\n",
    "device_type = 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "def get_batch(block_size=5):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    data = np.memmap(data_dir / 'train.bin', dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x = x.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x  = x.to(device)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0ba0ec-c42a-4c9f-8ffb-44998008dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_grams = [get_batch().tolist() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cea1862-bdd1-4a6b-9c9f-e7dd4b95e539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[263, 78, 338, 403, 77]\n",
      "[276, 72, 290, 68, 271]\n",
      "[83, 415, 199, 199, 52]\n",
      "[492, 79, 83, 264, 265]\n",
      "[14, 313, 266, 324, 434]\n",
      "[385, 378, 266, 359, 342]\n",
      "[89, 392, 79, 282, 388]\n",
      "[14, 221, 490, 440, 259]\n",
      "[46, 319, 12, 275, 311]\n",
      "[265, 298, 287, 65, 267]\n",
      "[472, 300, 69, 413, 260]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "i = 0\n",
    "for fg in chain.from_iterable(five_grams):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(fg)\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75fcf6fd-5cc8-47d3-9691-e0cb924dbc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix completion: 287452\n"
     ]
    }
   ],
   "source": [
    "def prefix_completion(starting_grams):\n",
    "    # Set to store all unique n-grams (including prefixes)\n",
    "    all_grams = set()\n",
    "\n",
    "    # Add all prefixes of each 4-gram\n",
    "    for gram in starting_grams:\n",
    "        for i in range(1, len(gram) + 1):\n",
    "            all_grams.add(tuple(gram[:i]))\n",
    "\n",
    "    # Calculate the difference between all grams and original 4-grams\n",
    "    completion = all_grams - set([tuple(gram) for gram in starting_grams])\n",
    "\n",
    "    return sorted(list(completion))\n",
    "\n",
    "# Example usage\n",
    "result = prefix_completion(chain.from_iterable(five_grams))\n",
    "print(f\"Prefix completion: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fce8db9-d53c-4d6a-88bf-28f42a747d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax, log_softmax\n",
    "\n",
    "#preds = tl_model(ngram_occurrences)[:, -1, :]\n",
    "#preds -= preds.mean(dim=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a771650d-e44f-4506-85f5-0d62e7d44742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6264, 0.6268, 0.6962,  ..., 0.5244, 0.8024, 0.9430])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ngram_markov.ngrams import kl_divergence\n",
    "\n",
    "\n",
    "base_logits = log_softmax(base_pred, dim=-1)\n",
    "mean_logits = log_softmax(preds.reshape(32, -1, 512).mean(dim=0), dim=-1)\n",
    "\n",
    "\n",
    "kl_divergence(mean_logits, base_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ad7d234-049b-4ff9-bdc3-702042aead49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ngram_occurrences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mngram_occurrences\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ngram_occurrences' is not defined"
     ]
    }
   ],
   "source": [
    "ngram_occurrences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcf13daf-c0a5-473a-a2e8-ddfe8f13b98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134217728"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512 ** 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ca10028-b7d0-41f5-91de-2c87c1c603f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 53_000\n",
    "model_path = Path('/media/External01/ngram-checkpoints/4layer_tinystories')\n",
    "\n",
    "vocab_size = 512\n",
    "\n",
    "\n",
    "model = load_tl_model(model_path / f'ckpt{epoch}.pt')\n",
    "model.eval()\n",
    "\n",
    "model_preds = []\n",
    "\n",
    "pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    unigram_preds = model(torch.arange(512).unsqueeze(1))[:, -1, :].cpu()\n",
    "\n",
    "all_bigrams = torch.cartesian_prod(torch.arange(vocab_size), torch.arange(vocab_size))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in all_bigrams.split(2048):\n",
    "        preds = model(batch.to('cuda'))[:, -1, :].cpu()\n",
    "        pred_list.append(preds)\n",
    "all_trigram_preds = torch.concat(pred_list)\n",
    "all_preds = torch.concat([unigram_preds, all_trigram_preds], dim=0)\n",
    "U, S, V = torch.svd(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be43667f-cf8f-400b-b583-a8ca06ead6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.matrix_rank(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6051bcaa-962c-4c3d-98a6-9deb74eb4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleWFA:\n",
    "    def __init__(self, U, S, V, all_preds, vocab_size, n_states):\n",
    "        self.U = U[:, :n_states]\n",
    "        self.S = S[:n_states]\n",
    "        self.V = V[:, :n_states]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_states = n_states\n",
    "\n",
    "        # Initialize WFA parameters\n",
    "        self.alpha = softmax(self.U[:vocab_size, :].sum(0), dim=0)  # Initial state distribution\n",
    "        self.omega = softmax(self.V[:vocab_size, :].sum(0), dim=0)  # Final state distribution\n",
    "        \n",
    "        # Project the original predictions into the lower-dimensional space\n",
    "        projected_preds = all_preds @ self.V\n",
    "        \n",
    "        # Separate unigram and bigram predictions\n",
    "        unigram_preds = projected_preds[:vocab_size]\n",
    "        bigram_preds = projected_preds[vocab_size:].reshape(vocab_size, vocab_size, n_states)\n",
    "\n",
    "        # Construct transition matrices\n",
    "        self.A = torch.zeros((vocab_size, n_states, n_states))\n",
    "        for i in range(vocab_size):\n",
    "            # bigram_preds[i] has shape (vocab_size, n_states)\n",
    "            # We need to transform this into (n_states, n_states)\n",
    "            state_transitions = self.V.T @ bigram_preds[i]  # Shape: (n_states, n_states)\n",
    "            self.A[i] = softmax(state_transitions, dim=1)\n",
    "\n",
    "    def evaluate(self, sequence):\n",
    "        state = self.alpha  # Initial state distribution\n",
    "        for symbol in sequence:\n",
    "            state = state @ self.A[symbol]  # Update state distribution\n",
    "        return state @ self.omega  # Final probability\n",
    "\n",
    "    def sample(self, max_length=100):\n",
    "        sequence = []\n",
    "        state = self.alpha  # Initial state distribution\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Compute symbol probabilities\n",
    "            symbol_probs = state @ self.A.sum(2).T  # Sum over next states\n",
    "            symbol = torch.multinomial(symbol_probs, 1).item()\n",
    "            sequence.append(symbol)\n",
    "            if symbol == 0:  # Assuming 0 is the end-of-sequence token\n",
    "                break\n",
    "            state = state @ self.A[symbol]  # Update state distribution\n",
    "\n",
    "        return sequence\n",
    "\n",
    "# all_preds is a Hankel matrix for a function over sequences\n",
    "#U, S, V = torch.svd(all_preds)\n",
    "vocab_size = 512\n",
    "n_states = torch.linalg.matrix_rank(all_preds).item()\n",
    "wfa = SimpleWFA(U, S, V, all_preds, vocab_size, n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e3008517-53ce-458b-aad4-712c5aecf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ee99e3a-3daf-4e4f-8629-d36d4860f7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos = torch.tensor([0], device='cuda')\n",
    "bos.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1e17f7c-63d2-4700-85ed-037813204e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6945d19a0a894f9a9746b98742900987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = model.generate(bos.unsqueeze(0), max_new_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a5ebc4a6-bf82-461d-9d7d-59c8e3021b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 279])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a103329d-8dc4-4e8e-89dd-7b8ec20227ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>Once upon a time, there was a little blue potato. She had a shiny green dress, and she she wanted to show it to all her friends.\\n\\nSo, the potato put the potato on the floor and told everyone about her hogse. Everyone was so excited to see the potato on the freezer and wait, but the little girl kept taking it out.\\n\\nFinally, the freezer was cold and sparkly. All the sparkles were turning the potato out of the fridge. Everyone was so happy and they kept taking more and more.\\n\\nThe little blue potato was no longer short or tired. From then on, she was even more curious about a special fat potato. The potato was the happiest she had ever been.<|endoftext|>'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0].cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f7ef332-e981-4b99-9b7e-3fea6584f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer/tinystories512\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f29adc99-0f6b-49a7-ae1a-8e07bf6f8f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'veray isg!3chF\\x12o� J� B rǭe shel'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(wfa.sample(max_length=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c06d25b-6b41-4cff-9e0f-31017fad79ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 512])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b1f81b1-c2b8-4f40-b55f-28610333b287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(wfa.A.transpose(1, 2) @ wfa.omega).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db375e-5343-4099-be42-8d58fc3eac11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing checkpoints:   0%|                                                                       | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing checkpoints:   1%|▊                                                              | 1/77 [00:14<17:52, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved results for checkpoint 100\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "def process_checkpoints(model_path, vocab_size, batch_size=2048):\n",
    "    \"\"\"\n",
    "    Process all checkpoints in the given path, calculate predictions, perform SVD,\n",
    "    and save results as numpy files.\n",
    "\n",
    "    Args:\n",
    "    model_path (str): Path to the directory containing model checkpoints\n",
    "    config: Model configuration\n",
    "    vocab_size (int): Size of the vocabulary\n",
    "    batch_size (int): Batch size for processing\n",
    "    n_components (int): Number of components to keep in SVD\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    model_path = Path(model_path)\n",
    "    \n",
    "    # Get all checkpoint files\n",
    "    checkpoint_files = sorted([f for f in os.listdir(model_path) if f.startswith('ckpt') and f.endswith('.pt')])\n",
    "\n",
    "    for ckpt_file in tqdm(checkpoint_files, desc=\"Processing checkpoints\"):\n",
    "        epoch = int(ckpt_file.split('ckpt')[1].split('.pt')[0])\n",
    "        \n",
    "        # Load the model\n",
    "        ckpt = torch.load(model_path / ckpt_file)\n",
    "        config = nanogpt_to_hooked_transformer_config(ckpt['model_args'])\n",
    "        tl_weights = convert_nanogpt_weights(ckpt['model'], config)\n",
    "        model = HookedTransformer(config)\n",
    "        model.load_state_dict(tl_weights)\n",
    "        model.eval()\n",
    "        model.to('cuda')\n",
    "\n",
    "        # Generate predictions\n",
    "        pred_list = []\n",
    "        unigram_inputs = torch.arange(vocab_size).unsqueeze(1).to('cuda')\n",
    "        with torch.no_grad():\n",
    "            unigram_preds = model(unigram_inputs)[:, -1, :].cpu()\n",
    "            \n",
    "            all_bigrams = torch.cartesian_prod(torch.arange(vocab_size), torch.arange(vocab_size))\n",
    "            for batch in all_bigrams.split(batch_size):\n",
    "                preds = model(batch.to('cuda'))[:, -1, :].cpu()\n",
    "                pred_list.append(preds)\n",
    "\n",
    "        all_trigram_preds = torch.cat(pred_list)\n",
    "        all_preds = torch.cat([unigram_preds, all_trigram_preds], dim=0)\n",
    "\n",
    "        # Perform SVD\n",
    "        U, S, V = torch.svd(all_preds)\n",
    "    \n",
    "        # Save results\n",
    "        save_dir = model_path / f'processed_ckpt{epoch}'\n",
    "        save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        np.save(save_dir / 'all_preds.npy', all_preds.numpy())\n",
    "        np.save(save_dir / 'U.npy', U.numpy())\n",
    "        np.save(save_dir / 'S.npy', S.numpy())\n",
    "        np.save(save_dir / 'V.npy', V.numpy())\n",
    "\n",
    "        print(f\"Processed and saved results for checkpoint {epoch}\")\n",
    "\n",
    "# Usage\n",
    "model_path = '/media/External01/ngram-checkpoints/4layer_tinystories'\n",
    "vocab_size = 512\n",
    "\n",
    "process_checkpoints(model_path, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "589c32e9-c954-40cb-a7bf-2cdf68940595",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_wfa = U[:, :n_states]\n",
    "S_wfa = S[:n_states]\n",
    "V_wfa = V[:, :n_states]\n",
    "       \n",
    "\n",
    "\n",
    "    # Initialize WFA parameters\n",
    "alpha = U[0, :]  # Initial state\n",
    "omega = V_wfa[0, :] * S_wfa  # Final state\n",
    "        \n",
    "        # Transition matrices (one per symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "22a1e37b-6628-4eaa-9060-241e7d177089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb9c33b1-7fed-4666-8975-e0f5b9c8f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.stack([\n",
    "    U[i*vocab_size:(i+1)*vocab_size, :] @ # 512 x 10\n",
    "    torch.diag(S_wfa) @ # 10 x 10\n",
    "    V_wfa[i*vocab_size:(i+1)*vocab_size, :].T\n",
    "            for i in range(vocab_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cee7a8cf-bd0a-442a-bc31-d36e8f7b09fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1200e+04, 3.4420e+03, 1.9294e+03, 1.3880e+03, 1.2805e+03, 1.2543e+03,\n",
       "        1.1475e+03, 1.0503e+03, 1.0072e+03, 9.9615e+02, 9.0286e+02, 8.8500e+02,\n",
       "        8.6497e+02, 8.3656e+02, 8.0855e+02, 7.8499e+02, 7.7931e+02, 7.2183e+02,\n",
       "        7.1082e+02, 7.0032e+02, 6.8252e+02, 6.5800e+02, 6.4312e+02, 6.3882e+02,\n",
       "        6.1603e+02, 6.1202e+02, 6.0251e+02, 5.9163e+02, 5.8398e+02, 5.7652e+02,\n",
       "        5.6503e+02, 5.5836e+02, 5.3562e+02, 5.3421e+02, 5.2170e+02, 5.0934e+02,\n",
       "        5.0336e+02, 4.9327e+02, 4.8829e+02, 4.7580e+02, 4.6953e+02, 4.5649e+02,\n",
       "        4.5277e+02, 4.4559e+02, 4.4255e+02, 4.3890e+02, 4.3068e+02, 4.2564e+02,\n",
       "        4.1645e+02, 4.0704e+02, 4.0103e+02, 3.9427e+02, 3.9204e+02, 3.8407e+02,\n",
       "        3.8080e+02, 3.7383e+02, 3.7059e+02, 3.6594e+02, 3.6220e+02, 3.5108e+02,\n",
       "        3.4595e+02, 3.3800e+02, 3.3532e+02, 3.2680e+02, 3.2173e+02, 3.1639e+02,\n",
       "        3.1121e+02, 3.0854e+02, 3.0516e+02, 3.0272e+02, 2.9451e+02, 2.9147e+02,\n",
       "        2.8783e+02, 2.8378e+02, 2.8226e+02, 2.7552e+02, 2.7193e+02, 2.7024e+02,\n",
       "        2.6428e+02, 2.6230e+02, 2.5724e+02, 2.5587e+02, 2.5096e+02, 2.4619e+02,\n",
       "        2.4595e+02, 2.4070e+02, 2.3725e+02, 2.3410e+02, 2.3128e+02, 2.2513e+02,\n",
       "        2.2357e+02, 2.2069e+02, 2.1740e+02, 2.1536e+02, 2.1407e+02, 2.1040e+02,\n",
       "        2.0691e+02, 2.0481e+02, 2.0263e+02, 1.9995e+02, 1.9536e+02, 1.9443e+02,\n",
       "        1.9364e+02, 1.9332e+02, 1.8830e+02, 1.8503e+02, 1.8481e+02, 1.8134e+02,\n",
       "        1.8025e+02, 1.7906e+02, 1.7620e+02, 1.7313e+02, 1.7181e+02, 1.7100e+02,\n",
       "        1.7009e+02, 1.6743e+02, 1.6597e+02, 1.6526e+02, 1.6157e+02, 1.5804e+02,\n",
       "        1.5755e+02, 1.5522e+02, 1.5414e+02, 1.5364e+02, 1.5018e+02, 1.4937e+02,\n",
       "        1.4854e+02, 1.4692e+02, 1.4485e+02, 1.4361e+02, 1.4254e+02, 1.4133e+02,\n",
       "        1.3957e+02, 1.3816e+02, 1.3679e+02, 1.3571e+02, 1.3458e+02, 1.3248e+02,\n",
       "        1.3129e+02, 1.2973e+02, 1.2926e+02, 1.2845e+02, 1.2631e+02, 1.2558e+02,\n",
       "        1.2546e+02, 1.2371e+02, 1.2348e+02, 1.2268e+02, 1.2004e+02, 1.1864e+02,\n",
       "        1.1752e+02, 1.1684e+02, 1.1615e+02, 1.1488e+02, 1.1462e+02, 1.1297e+02,\n",
       "        1.1197e+02, 1.1134e+02, 1.1040e+02, 1.0995e+02, 1.0821e+02, 1.0749e+02,\n",
       "        1.0669e+02, 1.0552e+02, 1.0419e+02, 1.0329e+02, 1.0300e+02, 1.0219e+02,\n",
       "        1.0137e+02, 1.0042e+02, 9.9596e+01, 9.8101e+01, 9.7722e+01, 9.6920e+01,\n",
       "        9.5793e+01, 9.5409e+01, 9.4722e+01, 9.4261e+01, 9.4136e+01, 9.2527e+01,\n",
       "        9.1668e+01, 9.1376e+01, 9.0488e+01, 9.0207e+01, 8.9669e+01, 8.8258e+01,\n",
       "        8.7549e+01, 8.7062e+01, 8.6756e+01, 8.6238e+01, 8.5793e+01, 8.5060e+01,\n",
       "        8.4945e+01, 8.3614e+01, 8.2955e+01, 8.2449e+01, 8.1585e+01, 8.1123e+01,\n",
       "        8.0438e+01, 7.9369e+01, 7.8412e+01, 7.7933e+01, 7.7587e+01, 7.7071e+01,\n",
       "        7.6495e+01, 7.5869e+01, 7.5567e+01, 7.4982e+01, 7.4328e+01, 7.4067e+01,\n",
       "        7.3668e+01, 7.3254e+01, 7.2594e+01, 7.2042e+01, 7.1791e+01, 7.0868e+01,\n",
       "        7.0499e+01, 7.0355e+01, 6.9321e+01, 6.9037e+01, 6.8562e+01, 6.7288e+01,\n",
       "        6.6777e+01, 6.6673e+01, 6.6559e+01, 6.6243e+01, 6.5753e+01, 6.5089e+01,\n",
       "        6.4659e+01, 6.4378e+01, 6.3646e+01, 6.2954e+01, 6.2794e+01, 6.2324e+01,\n",
       "        6.1904e+01, 6.1801e+01, 6.0996e+01, 6.0760e+01, 6.0392e+01, 6.0146e+01,\n",
       "        5.9114e+01, 5.9017e+01, 5.8752e+01, 5.8360e+01, 5.8124e+01, 5.7758e+01,\n",
       "        5.7521e+01, 5.6700e+01, 5.6409e+01, 5.6236e+01, 5.5985e+01, 5.5941e+01,\n",
       "        5.5197e+01, 5.5034e+01, 5.4367e+01, 5.4332e+01, 5.3871e+01, 5.2922e+01,\n",
       "        5.2564e+01, 5.2307e+01, 5.1823e+01, 5.1616e+01, 5.1312e+01, 5.1152e+01,\n",
       "        5.0870e+01, 5.0569e+01, 4.9988e+01, 4.9786e+01, 4.9548e+01, 4.9472e+01,\n",
       "        4.8993e+01, 4.8573e+01, 4.8310e+01, 4.8106e+01, 4.7527e+01, 4.6948e+01,\n",
       "        4.6613e+01, 4.6123e+01, 4.5936e+01, 4.5690e+01, 4.5283e+01, 4.5140e+01,\n",
       "        4.4903e+01, 4.4711e+01, 4.4033e+01, 4.3974e+01, 4.3864e+01, 4.3214e+01,\n",
       "        4.3041e+01, 4.2981e+01, 4.2718e+01, 4.2271e+01, 4.1844e+01, 4.1406e+01,\n",
       "        4.0971e+01, 4.0714e+01, 4.0457e+01, 4.0268e+01, 3.9875e+01, 3.9728e+01,\n",
       "        3.9517e+01, 3.9184e+01, 3.8930e+01, 3.8832e+01, 3.8672e+01, 3.8247e+01,\n",
       "        3.7981e+01, 3.7761e+01, 3.7456e+01, 3.7130e+01, 3.6880e+01, 3.6656e+01,\n",
       "        3.6063e+01, 3.5887e+01, 3.5807e+01, 3.5612e+01, 3.5006e+01, 3.4932e+01,\n",
       "        3.4790e+01, 3.4253e+01, 3.4097e+01, 3.3884e+01, 3.3635e+01, 3.3428e+01,\n",
       "        3.3239e+01, 3.2889e+01, 3.2692e+01, 3.2603e+01, 3.2215e+01, 3.1945e+01,\n",
       "        3.1712e+01, 3.1552e+01, 3.1260e+01, 3.1022e+01, 3.0738e+01, 3.0369e+01,\n",
       "        3.0214e+01, 3.0045e+01, 2.9786e+01, 2.9477e+01, 2.9414e+01, 2.9285e+01,\n",
       "        2.8820e+01, 2.8614e+01, 2.8383e+01, 2.8153e+01, 2.8113e+01, 2.7767e+01,\n",
       "        2.7686e+01, 2.7619e+01, 2.7374e+01, 2.7161e+01, 2.6898e+01, 2.6781e+01,\n",
       "        2.6361e+01, 2.6089e+01, 2.5939e+01, 2.5832e+01, 2.5446e+01, 2.5407e+01,\n",
       "        2.5171e+01, 2.5052e+01, 2.4667e+01, 2.4585e+01, 2.4363e+01, 2.4200e+01,\n",
       "        2.4072e+01, 2.3823e+01, 2.3464e+01, 2.3430e+01, 2.3089e+01, 2.3008e+01,\n",
       "        2.2888e+01, 2.2653e+01, 2.2469e+01, 2.2354e+01, 2.2243e+01, 2.1976e+01,\n",
       "        2.1655e+01, 2.1457e+01, 2.1121e+01, 2.1047e+01, 2.0909e+01, 2.0791e+01,\n",
       "        2.0598e+01, 2.0443e+01, 2.0234e+01, 2.0010e+01, 1.9873e+01, 1.9751e+01,\n",
       "        1.9519e+01, 1.9402e+01, 1.9281e+01, 1.9011e+01, 1.8860e+01, 1.8774e+01,\n",
       "        1.8551e+01, 1.8301e+01, 1.7982e+01, 1.7951e+01, 1.7696e+01, 1.7478e+01,\n",
       "        1.7375e+01, 1.7310e+01, 1.7117e+01, 1.6987e+01, 1.6679e+01, 1.6638e+01,\n",
       "        1.6286e+01, 1.6129e+01, 1.5824e+01, 1.5621e+01, 1.5509e+01, 1.5337e+01,\n",
       "        1.5169e+01, 1.4880e+01, 1.4817e+01, 1.4650e+01, 1.4537e+01, 1.4224e+01,\n",
       "        1.4128e+01, 1.3892e+01, 1.3802e+01, 1.3535e+01, 1.3458e+01, 1.3286e+01,\n",
       "        1.3246e+01, 1.2960e+01, 1.2787e+01, 1.2459e+01, 1.2337e+01, 1.2140e+01,\n",
       "        1.2094e+01, 1.1901e+01, 1.1761e+01, 1.1664e+01, 1.1437e+01, 1.1302e+01,\n",
       "        1.1116e+01, 1.0902e+01, 1.0743e+01, 1.0621e+01, 1.0483e+01, 1.0369e+01,\n",
       "        1.0233e+01, 1.0188e+01, 1.0077e+01, 9.9487e+00, 9.6547e+00, 9.5177e+00,\n",
       "        9.3401e+00, 9.1465e+00, 8.8885e+00, 8.8270e+00, 8.7448e+00, 8.6462e+00,\n",
       "        8.4495e+00, 8.1439e+00, 7.9180e+00, 7.8072e+00, 7.7201e+00, 7.5283e+00,\n",
       "        7.3628e+00, 7.1856e+00, 7.0467e+00, 6.7097e+00, 6.5545e+00, 6.4657e+00,\n",
       "        6.3961e+00, 6.2744e+00, 6.1110e+00, 5.8561e+00, 5.8127e+00, 5.6462e+00,\n",
       "        5.4855e+00, 5.2758e+00, 5.1172e+00, 4.9895e+00, 4.9180e+00, 4.6485e+00,\n",
       "        4.5960e+00, 4.5483e+00, 4.1763e+00, 4.0955e+00, 4.0190e+00, 3.9523e+00,\n",
       "        3.8207e+00, 3.7227e+00, 3.5415e+00, 3.1710e+00, 3.0082e+00, 2.9754e+00,\n",
       "        2.7921e+00, 2.6508e+00, 2.4031e+00, 2.2684e+00, 2.1524e+00, 1.9263e+00,\n",
       "        1.7783e+00, 1.6271e+00, 1.5259e+00, 1.3282e+00, 1.2144e+00, 1.0485e+00,\n",
       "        8.8473e-01, 7.8188e-01, 5.8284e-01, 3.5966e-01, 3.0765e-01, 2.1928e-01,\n",
       "        3.3624e-02, 2.3387e-03])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc2b52ba-0fa8-4871-b717-67b847eeca41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(210.9469, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(base_pred - preds[6]).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c13600bd-0b5d-4e01-bc6a-685aa423f44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(softmax(preds[0] - preds[0].mean(), dim=0), softmax(preds[0], dim=0), atol=1.e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dbfb0b8e-0ecd-4fc8-9818-62144cff6b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.3447e-06, device='cuda:0', grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds - preds.mean(dim=0, keepdims=True)).mean(dim=0).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a19d4-e58c-4fd8-a276-cf9760449af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
