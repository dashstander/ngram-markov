{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c32e9e-ccd6-4528-a799-244114241a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from plotnine import (\n",
    "    ggplot,\n",
    "    geom_point, \n",
    "    geom_histogram, \n",
    "    geom_line,\n",
    "    geom_ribbon,\n",
    "    qplot, \n",
    "    coord_fixed, \n",
    "    aes, \n",
    "    facet_wrap, \n",
    "    labs,\n",
    "    scale_x_log10,\n",
    "    scale_y_log10\n",
    ")\n",
    "import polars as pl\n",
    "import torch\n",
    "\n",
    "from tokengrams import MemmapIndex, InMemoryIndex\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens import HookedTransformerConfig\n",
    "\n",
    "from ngram_markov.hooked_transformer import HookedTransformer\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "\n",
    "\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import rustworkx as rx\n",
    "\n",
    "\n",
    "from ngram_markov.model import GPT, GPTConfig\n",
    "from ngram_markov.utils import create_ngrams, nanogpt_to_hooked_transformer_config, convert_nanogpt_weights\n",
    "import einops\n",
    "import torch\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9579f-df1b-4374-8961-8b818678de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def efron_stein_decomposition(tensor):\n",
    "    \"\"\"\n",
    "    Perform Efron-Stein decomposition on a 3D tensor representing a single output logit.\n",
    "    \n",
    "    :param tensor: numpy array of shape (512, 512, 512)\n",
    "    :return: tuple of (zeroth_order, first_order, second_order, third_order)\n",
    "    \"\"\"\n",
    "    # Zeroth-order effect (mean)\n",
    "    zeroth_order = np.mean(tensor)\n",
    "    \n",
    "    # First-order effects\n",
    "    first_order = {\n",
    "        0: np.mean(tensor, axis=(1, 2)) - zeroth_order,\n",
    "        1: np.mean(tensor, axis=(0, 2)) - zeroth_order,\n",
    "        2: np.mean(tensor, axis=(0, 1)) - zeroth_order\n",
    "    }\n",
    "    \n",
    "    # Second-order effects\n",
    "    second_order = {\n",
    "        (0, 1): np.mean(tensor, axis=2) - first_order[0][:, np.newaxis] - first_order[1][np.newaxis, :] - zeroth_order,\n",
    "        (0, 2): np.mean(tensor, axis=1) - first_order[0][:, np.newaxis] - first_order[2][np.newaxis, :] - zeroth_order,\n",
    "        (1, 2): np.mean(tensor, axis=0) - first_order[1][:, np.newaxis] - first_order[2][np.newaxis, :] - zeroth_order\n",
    "    }\n",
    "    \n",
    "    # Third-order effect\n",
    "    third_order = (tensor - \n",
    "                   zeroth_order - \n",
    "                   first_order[0][:, np.newaxis, np.newaxis] -\n",
    "                   first_order[1][np.newaxis, :, np.newaxis] -\n",
    "                   first_order[2][np.newaxis, np.newaxis, :] -\n",
    "                   second_order[(0, 1)][:, :, np.newaxis] -\n",
    "                   second_order[(0, 2)][:, np.newaxis, :] -\n",
    "                   second_order[(1, 2)][np.newaxis, :, :])\n",
    "    \n",
    "    return zeroth_order, first_order, second_order, third_order\n",
    "\n",
    "def check_orthogonality(first, second, third, tolerance=1e-6):\n",
    "    def expand_to_full(component, order, shape):\n",
    "        full = np.zeros(shape)\n",
    "        new_dims = tuple([i for i in range(3) if i not in order])\n",
    "        if len(order) < len(shape):\n",
    "            full += np.expand_dims(component, axis=new_dims)\n",
    "        else:\n",
    "            full = component\n",
    "        return full\n",
    "\n",
    "    shape = (512, 512, 512)\n",
    "    components = [\n",
    "        expand_to_full(first[0], (0,), shape),\n",
    "        expand_to_full(first[1], (1,), shape),\n",
    "        expand_to_full(first[2], (2,), shape),\n",
    "        expand_to_full(second[(0,1)], (0,1), shape),\n",
    "        expand_to_full(second[(0,2)], (0,2), shape),\n",
    "        expand_to_full(second[(1,2)], (1,2), shape),\n",
    "        third\n",
    "    ]\n",
    "    \n",
    "    # Flatten each component\n",
    "    flat_components = [comp.flatten() for comp in components]\n",
    "    \n",
    "    for i in tqdm(range(len(flat_components)), desc=\"Checking orthogonality\"):\n",
    "        for j in range(i+1, len(flat_components)):\n",
    "            dot_product = np.dot(flat_components[i], flat_components[j])\n",
    "            assert np.abs(dot_product) < tolerance, f\"Components {i} and {j} are not orthogonal. Dot product: {dot_product}\"\n",
    "    \n",
    "    print(\"All components are orthogonal in the function space.\")\n",
    "\n",
    "def check_reconstruction(tensor, zeroth_order, first_order, second_order, third_order):\n",
    "    reconstructed = (zeroth_order + \n",
    "                     first_order[0][:, np.newaxis, np.newaxis] +\n",
    "                     first_order[1][np.newaxis, :, np.newaxis] +\n",
    "                     first_order[2][np.newaxis, np.newaxis, :] +\n",
    "                     second_order[(0,1)][:, :, np.newaxis] +\n",
    "                     second_order[(0,2)][:, np.newaxis, :] +\n",
    "                     second_order[(1,2)][np.newaxis, :, :] +\n",
    "                     third_order)\n",
    "    max_error = np.max(np.abs(tensor - reconstructed))\n",
    "    print(f\"Maximum reconstruction error: {max_error}\")\n",
    "    assert np.allclose(tensor, reconstructed), \"Reconstruction failed\"\n",
    "\n",
    "def check_variances(tensor, first_order, second_order, third_order):\n",
    "    total_var = np.var(tensor)\n",
    "    component_vars = (np.var(first_order[0]) + np.var(first_order[1]) + np.var(first_order[2]) +\n",
    "                      np.var(second_order[(0,1)]) + np.var(second_order[(0,2)]) + np.var(second_order[(1,2)]) +\n",
    "                      np.var(third_order))\n",
    "    print(f\"Total variance: {total_var}\")\n",
    "    print(f\"Sum of component variances: {component_vars}\")\n",
    "    assert np.allclose(total_var, component_vars), \"Variance decomposition failed\"\n",
    "\n",
    "\n",
    "ngram_n = 3\n",
    "epoch = 53_000\n",
    "num_tokens = 512\n",
    "\n",
    "mm_array = np.memmap(\n",
    "    f'ngram_{ngram_n}_outputs_epoch_{epoch}.npy',\n",
    "    dtype='float32', \n",
    "    mode='r',\n",
    "    shape=(num_tokens, num_tokens, num_tokens, num_tokens)\n",
    ")\n",
    "\n",
    "data = np.copy(mm_array[..., 2])\n",
    "\n",
    "\n",
    "# Perform Efron-Stein decomposition\n",
    "zeroth_order, first_order, second_order, third_order = efron_stein_decomposition(data)\n",
    "\n",
    "# Run checks\n",
    "check_orthogonality(first_order, second_order, third_order)\n",
    "check_reconstruction(data, zeroth_order, first_order, second_order, third_order)\n",
    "check_variances(data, first_order, second_order, third_order)\n",
    "\n",
    "print(\"All checks passed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ecc76e6-ea58-4c1a-9c38-e00a05cd3772",
   "metadata": {},
   "outputs": [],
   "source": [
    "zo_ground_truth =  -8. / 9\n",
    "\n",
    "fo_ground_truth = {\n",
    "    0: np.array([-1., -1., 0.]),\n",
    "    1: np.array([0., -1., -1.]),\n",
    "    2: np.array([-1., 0., -1.]),\n",
    "}\n",
    "\n",
    "so_ground_truth = {\n",
    "    (0, 1): np.eye(3) * 4.,\n",
    "    (0, 2): np.zeros((3, 3)),\n",
    "    (1, 2): np.eye(3) * 4.\n",
    "    \n",
    "}\n",
    "\n",
    "to_ground_truth = np.zeros((3, 3, 3))\n",
    "to_ground_truth[0, 1, 2] = 2.\n",
    "to_ground_truth[2, 0, 1] = 2.\n",
    "to_ground_truth[1, 2, 0] = 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6472ef8c-85ef-4039-bf7d-daaf4c737429",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = ( \n",
    "    fo_ground_truth[0][:, None, None] + fo_ground_truth[1][None, :, None] + fo_ground_truth[2][None, None, :] + \n",
    "    so_ground_truth[(0,1)][:, :, None] + so_ground_truth[(0,2)][:, None, :] + so_ground_truth[(1,2)][None, :, :] + \n",
    "    to_ground_truth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3df9b92-ff9c-47a9-aca5-e848fda1fc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(ground_truth, test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "24487ba8-e363-42f1-820c-1268ca251a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementation matches manual calculation.\n",
      "All component means (except zeroth-order) are zero.\n",
      "All components are orthogonal in the function space.\n",
      "Reconstruction successful.\n",
      "Variance decomposition successful.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "First-order effect mismatch for position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 178\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(zeroth, zo_ground_truth), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZeroth-order effect mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m first\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# This fails for an interesting normalization reason. The output of `first` is fo_ground_truth[i] / 3\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(first[k], fo_ground_truth[k]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst-order effect mismatch for position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m second\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# this fails for a more complicated reason. I expect the output of second[(0, 2)] to be all zeros, and second[(0, 1)] to have\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# just 4s on the diagonal, but this is what they look like:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m#    [-1.55555556,  2.44444444, -0.88888889],\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m#    [-0.88888889, -1.55555556,  2.44444444]])}\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(second[k], manual_second[k]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSecond-order effect mismatch for positions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: First-order effect mismatch for position 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "zo_ground_truth = 8. / 9\n",
    "\n",
    "fo_ground_truth = {\n",
    "    0: np.array([-1., -1., 0.]),\n",
    "    1: np.array([0., -1., -1.]),\n",
    "    2: np.array([-1., 0., -1.])\n",
    "}\n",
    "\n",
    "so_ground_truth = {\n",
    "    (0, 1): np.eye(3) * 4.,\n",
    "    (0, 2): np.zeros((3, 3)),\n",
    "    (1, 2): np.eye(3) * 4.\n",
    "    \n",
    "}\n",
    "\n",
    "to_ground_truth = np.zeros((3, 3, 3))\n",
    "to_ground_truth[0, 1, 2] = 2.\n",
    "to_ground_truth[2, 0, 1] = 2.\n",
    "to_ground_truth[1, 2, 0] = 2.\n",
    "\n",
    "ground_truth = (\n",
    "    fo_ground_truth[0][:, None, None] + fo_ground_truth[1][None, :, None] + fo_ground_truth[2][None, None, :] + \n",
    "    so_ground_truth[(0,1)][:, :, None] + so_ground_truth[(0,2)][:, None, :] + so_ground_truth[(1,2)][None, :, :] + \n",
    "    to_ground_truth\n",
    ")\n",
    "\n",
    "\n",
    "def simple_discrete_function(x0, x1, x2):\n",
    "    \"\"\"\n",
    "    A simple function with discrete inputs (0 or 1 for each position).\n",
    "    This function includes main effects, two-way interactions, and a three-way interaction.\n",
    "    \"\"\"\n",
    "    first_order = fo_ground_truth[0][x0] + fo_ground_truth[1][x1] + fo_ground_truth[2][x2]\n",
    "    second_order = so_ground_truth[(0, 1)][x0, x1] + so_ground_truth[(0, 2)][x0, x1] + so_ground_truth[(1, 2)][x1, x2]\n",
    "    third_order = to_ground_truth[x0, x1, x2]\n",
    "    return first_order + second_order + third_order\n",
    "\n",
    "def create_test_tensor(size=3):\n",
    "    \"\"\"\n",
    "    Create a test tensor using all possible combinations of 0 and 1 for each position.\n",
    "    \"\"\"\n",
    "    tensor = np.zeros((size, size, size))\n",
    "    for x, y, z in itertools.product(range(size), repeat=3):\n",
    "        tensor[x, y, z] = simple_discrete_function(x, y, z)\n",
    "    return tensor\n",
    "\n",
    "def efron_stein_decomposition(tensor):\n",
    "    # Implement your Efron-Stein decomposition here\n",
    "    # This is where you'll put your actual implementation\n",
    "    pass\n",
    "\n",
    "\n",
    "def check_means(zeroth, first, second, third):\n",
    "    assert np.allclose(np.mean(first[0]), 0)\n",
    "    assert np.allclose(np.mean(first[1]), 0)\n",
    "    assert np.allclose(np.mean(first[2]), 0)\n",
    "    assert np.allclose(np.mean(second[(0,1)]), 0)\n",
    "    assert np.allclose(np.mean(second[(0,2)]), 0)\n",
    "    assert np.allclose(np.mean(second[(1,2)]), 0)\n",
    "    assert np.allclose(np.mean(third), 0)\n",
    "    print(\"All component means (except zeroth-order) are zero.\")\n",
    "\n",
    "\n",
    "def expand_to_full(component, order, shape):\n",
    "    full = np.zeros(shape)\n",
    "    new_dims = tuple([i for i in range(3) if i not in order])\n",
    "    if len(order) < len(shape):\n",
    "        full += np.expand_dims(component, axis=new_dims)\n",
    "    else:\n",
    "        full = component\n",
    "    return full\n",
    "\n",
    "\n",
    "\n",
    "def check_orthogonality(first, second, third, tolerance=1e-6):\n",
    "  \n",
    "\n",
    "    shape = (3, 3, 3)\n",
    "    components = [\n",
    "        expand_to_full(first[0], (0,), shape),\n",
    "        expand_to_full(first[1], (1,), shape),\n",
    "        expand_to_full(first[2], (2,), shape),\n",
    "        expand_to_full(second[(0,1)], (0,1), shape),\n",
    "        expand_to_full(second[(0,2)], (0,2), shape),\n",
    "        expand_to_full(second[(1,2)], (1,2), shape),\n",
    "        third\n",
    "    ]\n",
    "    \n",
    "    # Flatten each component\n",
    "    flat_components = [comp.flatten() for comp in components]\n",
    "    \n",
    "    for i in range(len(flat_components)):\n",
    "        for j in range(i+1, len(flat_components)):\n",
    "            dot_product = np.dot(flat_components[i], flat_components[j])\n",
    "            assert np.abs(dot_product) < tolerance, f\"Components {i} and {j} are not orthogonal. Dot product: {dot_product}\"\n",
    "    \n",
    "    print(\"All components are orthogonal in the function space.\")\n",
    "\n",
    "\n",
    "\n",
    "def check_reconstruction(tensor, zeroth, first, second, third):\n",
    "    reconstructed = (zeroth + \n",
    "                     first[0][:, None, None] + first[1][None, :, None] + first[2][None, None, :] + \n",
    "                     second[(0,1)][:, :, None] + second[(0,2)][:, None, :] + second[(1,2)][None, :, :] + \n",
    "                     third)\n",
    "    assert np.allclose(tensor, reconstructed), \"Reconstruction failed\"\n",
    "    print(\"Reconstruction successful.\")\n",
    "\n",
    "\n",
    "def check_variances(tensor, zeroth, first, second, third):\n",
    "    total_var = np.var(tensor)\n",
    "    component_vars = (np.var(first[0]) + np.var(first[1]) + np.var(first[2]) +\n",
    "                      np.var(second[(0,1)]) + np.var(second[(0,2)]) + np.var(second[(1,2)]) +\n",
    "                      np.var(third))\n",
    "    assert np.allclose(total_var, component_vars), \"Variance decomposition failed\"\n",
    "    print(\"Variance decomposition successful.\")\n",
    "\n",
    "def manual_calculation(tensor):\n",
    "    \"\"\"\n",
    "    Manually calculate the Efron-Stein decomposition for verification.\n",
    "    \"\"\"\n",
    "    mean = np.mean(tensor)\n",
    "    \n",
    "    first_order = {\n",
    "        0: np.mean(tensor, axis=(1, 2)) - mean,\n",
    "        1: np.mean(tensor, axis=(0, 2)) - mean,\n",
    "        2: np.mean(tensor, axis=(0, 1)) - mean\n",
    "    }\n",
    "    \n",
    "    second_order = {\n",
    "        (0, 1): np.mean(tensor, axis=2) - first_order[0][:, np.newaxis] - first_order[1][np.newaxis, :] - mean,\n",
    "        (0, 2): np.mean(tensor, axis=1) - first_order[0][:, np.newaxis] - first_order[2][np.newaxis, :] - mean,\n",
    "        (1, 2): np.mean(tensor, axis=0) - first_order[1][:, np.newaxis] - first_order[2][np.newaxis, :] - mean\n",
    "    }\n",
    "    \n",
    "    third_order = (tensor - \n",
    "                   mean - \n",
    "                   first_order[0][:, np.newaxis, np.newaxis] -\n",
    "                   first_order[1][np.newaxis, :, np.newaxis] -\n",
    "                   first_order[2][np.newaxis, np.newaxis, :] -\n",
    "                   second_order[(0, 1)][:, :, np.newaxis] -\n",
    "                   second_order[(0, 2)][:, np.newaxis, :] -\n",
    "                   second_order[(1, 2)][np.newaxis, :, :])\n",
    "    \n",
    "    return mean, first_order, second_order, third_order\n",
    "\n",
    "# Run tests\n",
    "test_tensor = create_test_tensor()\n",
    "\n",
    "# Your implementation\n",
    "#zeroth, first, second, third = efron_stein_decomposition(test_tensor)\n",
    "\n",
    "# Manual calculation for verification\n",
    "zeroth, first, second, third = manual_calculation(test_tensor)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Implementation matches manual calculation.\")\n",
    "\n",
    "# These all pass except for the orthogonality one, but I actually am not sure that's implemented correctly. Don't worry about it.\n",
    "check_means(zeroth, first, second, third)\n",
    "check_orthogonality(first, second, third) # This fails\n",
    "check_reconstruction(test_tensor, zeroth, first, second, third)\n",
    "check_variances(test_tensor, zeroth, first, second, third)\n",
    "\n",
    "\n",
    "# Compare implementation with \"ground truth\"\n",
    "assert np.allclose(ground_truth, test_tensor) # we implemented \"basic function\" correctly\n",
    "\n",
    "assert np.allclose(zeroth, zo_ground_truth), \"Zeroth-order effect mismatch\"\n",
    "\n",
    "for k in first.keys():\n",
    "    # This fails for an interesting normalization reason. The output of `first` is fo_ground_truth[i] / 3\n",
    "    assert np.allclose(first[k], fo_ground_truth[k]), f\"First-order effect mismatch for position {k}\"\n",
    "\n",
    "for k in second.keys():\n",
    "    # this fails for a more complicated reason. I expect the output of second[(0, 2)] to be all zeros, and second[(0, 1)] to have\n",
    "    # just 4s on the diagonal, but this is what they look like:\n",
    "    # {(0,1): array([[ 2.44444444, -0.88888889, -1.55555556],\n",
    "    #    [-1.55555556,  2.44444444, -0.88888889],\n",
    "    #    [-0.88888889, -1.55555556,  2.44444444]]),\n",
    "    # (0,2): array([[-0.22222222, -0.22222222,  0.44444444],\n",
    "    #    [ 0.44444444, -0.22222222, -0.22222222],\n",
    "    #    [-0.22222222,  0.44444444, -0.22222222]]),\n",
    "    # (1,2): array([[ 2.44444444, -0.88888889, -1.55555556],\n",
    "    #    [-1.55555556,  2.44444444, -0.88888889],\n",
    "    #    [-0.88888889, -1.55555556,  2.44444444]])}\n",
    "    assert np.allclose(second[k], manual_second[k]), f\"Second-order effect mismatch for positions {k}\"\n",
    "\n",
    "# Haven't really looked at this yet\n",
    "assert np.allclose(third, manual_third), \"Third-order effect mismatch\"\n",
    "\n",
    "print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "db35bc07-e8a2-48aa-b32b-d7300c989a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = np.zeros((3, 3, 3))\n",
    "test1 = np.ones((3,))\n",
    "test2 = np.ones((3, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a4af95ff-1593-4718-ae7d-b0b6b8be1305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.]],\n",
       "\n",
       "       [[1.]],\n",
       "\n",
       "       [[1.]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(test1, axis=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b5733bfc-768f-4f20-a5a6-674a10a8a0eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' (543767458.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[86], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    full[(slice(None), slice(None), 1]\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "full[(slice(None), slice(None), 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a8e8a4a-225f-4efb-aee9-5a83739af746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.33333333, -0.33333333,  0.66666667])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (3, 3, 3)\n",
    "expand_to_full(first[0], (0,), shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c47814-66af-424d-8e2c-e319d77548cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d130c38d-2713-41cb-aad5-40bad85cfc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.66666667, -0.33333333, -0.33333333])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_to_full(first[1], (1,), shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51d16157-7899-46d4-9f02-11ae6328fc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 0.],\n",
       "       [0., 4., 0.],\n",
       "       [0., 0., 4.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "so_ground_truth[(0, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd795ce-6d76-492b-9593-8b1ce6c71768",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_n = 3\n",
    "epoch = 53_000\n",
    "\n",
    "mm_array = np.memmap(\n",
    "    f'ngram_{ngram_n}_outputs_epoch_{epoch}.npy',\n",
    "    dtype='float32', \n",
    "    mode='r',\n",
    "    shape=(512, 512, 512, 512)\n",
    ")\n",
    "\n",
    "num_tokens = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a76a723-f313-4582-9c88-67d6eb4a3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zeroth_order = mm_array.mean(axis=(0, 1, 2))\n",
    "#first_order = {\n",
    "#    0: np.mean(mm_array, axis=(1, 2)) - zeroth_order[np.newaxis, :],\n",
    "#    1: np.mean(mm_array, axis=(0, 2)) - zeroth_order[np.newaxis, :],\n",
    "#    2: np.mean(mm_array, axis=(1, 2)) - zeroth_order[np.newaxis, :]\n",
    "#}\n",
    "#second_order = {\n",
    "#    (0, 1): np.mean(mm_array, axis=2) - first_order[0][:, np.newaxis] - first_order[1][np.newaxis, :] - zeroth_order[np.newaxis, np.newaxis, :],\n",
    "#    (0, 2): np.mean(mm_array, axis=1) - first_order[0][:, np.newaxis] - first_order[2][np.newaxis, :] - zeroth_order[np.newaxis, np.newaxis, :],\n",
    "#    (1, 2): np.mean(mm_array, axis=0) - first_order[1][:, np.newaxis] - first_order[2][np.newaxis, :] - zeroth_order[np.newaxis, np.newaxis, :]\n",
    "#}\n",
    "\n",
    "partial_es = torch.load('partial_efron_stein.pt')\n",
    "\n",
    "zeroth_order = partial_es['zeroth_order'].numpy()\n",
    "first_order = {k: v.numpy() for k, v in partial_es['first_order'].items()}\n",
    "second_order = {k: v.numpy() for k, v in partial_es['second_order'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8deeb4-8fa3-4479-bed4-0338c6365e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "674c7837-eeee-4ae1-afa6-8773ba54873d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12062385, 3.783763, 4.334224]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.var(v[..., 24], axis=(0, 1)) for v in second_order.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc9afbb6-46a5-4232-a546-e9b656c956ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_order = np.memmap(\n",
    "    'third_order_results.npy',\n",
    "    dtype='float32', \n",
    "    mode='r',\n",
    "    shape=(512, 512, 512, 512)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db1d65ef-77bc-43e7-942b-45b77cd5a439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.018879"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ec1645f-d06c-4b8d-8c2f-4886995c0d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.4358907"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.var(mm_array[..., 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12902ece-132a-486a-a017-8a15ef445e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save({\n",
    "#        'zeroth_order': torch.from_numpy(zeroth_order),\n",
    "#        'first_order': {k: torch.from_numpy(v) for k, v in first_order.items()},\n",
    "#        'second_order': {k: torch.from_numpy(v) for k, v in second_order.items()}\n",
    "#}, 'partial_efron_stein.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7108a9f-99ea-40aa-b8e7-c3f554d24f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 16/16 [16:24<00:00, 61.56s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_third_order(mm_array, zeroth_order, first_order, second_order, output_file):\n",
    "    # Create a memory-mapped array for the output\n",
    "    third_order = np.memmap(output_file, dtype='float32', mode='w+', \n",
    "                            shape=(512, 512, 512, 512))\n",
    "\n",
    "    # Process the data in chunks\n",
    "    chunk_size = 32  # Adjust this based on your available memory\n",
    "    for i in tqdm(range(0, 512, chunk_size)):\n",
    "        for j in range(0, 512, chunk_size):\n",
    "            for k in range(0, 512, chunk_size):\n",
    "                # Load a chunk of the input array\n",
    "                chunk = mm_array[i:i+chunk_size, j:j+chunk_size, k:k+chunk_size, :]\n",
    "                \n",
    "                # Perform the calculation on the chunk\n",
    "                result = (\n",
    "                    chunk\n",
    "                    - zeroth_order[np.newaxis, np.newaxis, np.newaxis, :]\n",
    "                    - first_order[0][i:i+chunk_size, np.newaxis, np.newaxis, :]\n",
    "                    - first_order[1][np.newaxis, j:j+chunk_size, np.newaxis, :]\n",
    "                    - first_order[2][np.newaxis, np.newaxis, k:k+chunk_size, :]\n",
    "                    - second_order[(0, 1)][i:i+chunk_size, j:j+chunk_size, np.newaxis, :]\n",
    "                    - second_order[(0, 2)][i:i+chunk_size, np.newaxis, k:k+chunk_size, :]\n",
    "                    - second_order[(1, 2)][np.newaxis, j:j+chunk_size, k:k+chunk_size, :]\n",
    "                )\n",
    "                \n",
    "                # Write the result to the memory-mapped array\n",
    "                third_order[i:i+chunk_size, j:j+chunk_size, k:k+chunk_size, :] = result\n",
    "\n",
    "    # Flush to ensure all data is written to disk\n",
    "    third_order.flush()\n",
    "\n",
    "    return third_order\n",
    "\n",
    "# Usage\n",
    "output_file = 'third_order_results.npy'\n",
    "third_order = calculate_third_order(mm_array, zeroth_order, first_order, second_order, output_file)\n",
    "\n",
    "# To load and use the results later:\n",
    "# third_order = np.memmap('third_order_results.npy', dtype='float32', mode='r', shape=(512, 512, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a3b3a7d-d80d-43fc-b584-c2f0c8c86c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-order effect (dim 0) mean magnitude: 3.24462890625\n",
      "First-order effect (dim 1) mean magnitude: 3.2683374881744385\n",
      "First-order effect (dim 2) mean magnitude: 3.24462890625\n",
      "Second-order effect (dims 0,1) mean magnitude: 3.2460999488830566\n",
      "Second-order effect (dims 0,2) mean magnitude: 3.7644736766815186\n",
      "Second-order effect (dims 1,2) mean magnitude: 3.81758189201355\n",
      "Third-order effect mean: -3.159119129180908\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 256. GiB for an array with shape (512, 512, 512, 512) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThird-order effect mean: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthird_order\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Variance explained\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m total_var \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmm_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m first_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39mvar(eff) \u001b[38;5;28;01mfor\u001b[39;00m eff \u001b[38;5;129;01min\u001b[39;00m first_order\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     11\u001b[0m second_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39mvar(eff) \u001b[38;5;28;01mfor\u001b[39;00m eff \u001b[38;5;129;01min\u001b[39;00m second_order\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/workspace/ngram-markov/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3785\u001b[0m, in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m   3783\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   3784\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3785\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_var(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof,\n\u001b[1;32m   3788\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/workspace/ngram-markov/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:173\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    168\u001b[0m     arrmean \u001b[38;5;241m=\u001b[39m arrmean \u001b[38;5;241m/\u001b[39m rcount\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# not a scalar.\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m x \u001b[38;5;241m=\u001b[39m asanyarray(\u001b[43marr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marrmean\u001b[49m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, (nt\u001b[38;5;241m.\u001b[39mfloating, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[1;32m    176\u001b[0m     x \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mmultiply(x, x, out\u001b[38;5;241m=\u001b[39mx)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 256. GiB for an array with shape (512, 512, 512, 512) and data type float32"
     ]
    }
   ],
   "source": [
    "\n",
    "# Analysis\n",
    "for i in range(3):\n",
    "    print(f\"First-order effect (dim {i}) mean magnitude: {np.abs(first_order[i]).mean()}\")\n",
    "for (i, j) in second_order:\n",
    "    print(f\"Second-order effect (dims {i},{j}) mean magnitude: {np.abs(second_order[(i,j)]).mean()}\")\n",
    "print(f\"Third-order effect mean: {third_order.mean()}\")\n",
    "\n",
    "# Variance explained\n",
    "total_var = np.var(mm_array)\n",
    "first_var = sum(np.var(eff) for eff in first_order.values())\n",
    "second_var = sum(np.var(eff) for eff in second_order.values())\n",
    "third_var = np.var(third_order)\n",
    "\n",
    "print(\"\\nVariance explained:\")\n",
    "print(f\"First order: {first_var / total_var * 100:.2f}%\")\n",
    "print(f\"Second order: {second_var / total_var * 100:.2f}%\")\n",
    "print(f\"Third order: {third_var / total_var * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f33a35-ddc4-4d21-9581-104d93180165",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vars = np.array([np.var(mm_array[..., i]) for i in range(512)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb10169-e62e-4988-a05e-62f3e1adcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_order = (\n",
    "    mm_array \n",
    "    - zeroth_order[np.newaxis, np.newaxis, np.newaxis, :]\n",
    "    - first_order[0][:, np.newaxis, np.newaxis, :] # (cond_0, out) -> (cond_0, 1, 1, out)\n",
    "    - first_order[1][np.newaxis, :, np.newaxis, :] # (cond_1, out) -> (1, cond_1, 1, out)\n",
    "    - first_order[2][np.newaxis, np.newaxis, :, :] # (cond_2, out) -> (1, 1, cond_2, out)\n",
    "    - second_order[(0, 1)][:, :, np.newaxis, :] # (cond_0, cond_1, out) -> (cond_0, cond_1, 1, out)\n",
    "    - second_order[(0, 2)][:, np.newaxis, :, :]  # (cond_0, cond_2, out) -> (cond_0, 1, cond_2, out)\n",
    "    - second_order[(1, 2)][np.newaxis, :, :, :]  # (cond_1, cond_2, out) -> (1, cond_1, cond_2, out)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f237a6c-88d4-4209-a862-2d3a6f12e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('partial_efron_stein.npy', {\"zeroth\": zeroth_order, \"first\": first_order}, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dcafd-4ef9-430a-9bb2-3f054dac443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_order = {\n",
    "    (0, 1): np.mean(mm_array, axis=2) - first_order[0][:, np.new_axis] - first_order[1][np.newaxis, :] - zeroth_order,\n",
    "    (0, 2): np.mean(mm_array, axis=1) - first_order[0][:, np.new_axis] - first_order[2][np.newaxis, :] - zeroth_order,\n",
    "    (1, 2): np.mean(mm_array, axis=0) - first_order[1][:, np.newaxis] - first_order[2][np.newaxis, :] - zeroth_order\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82679e91-dd65-457e-b08b-3fa95843aee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating 0th Order\n",
      "Calculating 1st Order\n",
      "Calculating 2nd Order\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def efron_stein_decomposition(mm_array):\n",
    "    # Zeroth-order effect\n",
    "    print(f'Calculating 0th Order')\n",
    "    zeroth_order = np.mean(mm_array)\n",
    "\n",
    "    print(f'Calculating 1st Order')\n",
    "    # First-order effects\n",
    "    first_order = {\n",
    "        0: np.mean(mm_array, axis=(1, 2, 3)) - zeroth_order,\n",
    "        1: np.mean(mm_array, axis=(0, 2, 3)) - zeroth_order,\n",
    "        2: np.mean(mm_array, axis=(0, 1, 3)) - zeroth_order\n",
    "    }\n",
    "\n",
    "    print(f'Calculating 2nd Order')\n",
    "    # Second-order effects\n",
    "    second_order = {\n",
    "        (0, 1): np.mean(mm_array, axis=(2, 3)) - first_order[0][:, np.newaxis] - first_order[1][np.newaxis, :] - zeroth_order,\n",
    "        (0, 2): np.mean(mm_array, axis=(1, 3)) - first_order[0][:, np.newaxis] - first_order[2][np.newaxis, :] - zeroth_order,\n",
    "        (1, 2): np.mean(mm_array, axis=(0, 3)) - first_order[1][:, np.newaxis] - first_order[2][np.newaxis, :] - zeroth_order\n",
    "    }\n",
    "\n",
    "    print(f'Calculating 3rd Order')\n",
    "    # Third-order effects\n",
    "    third_order = (\n",
    "        mm_array \n",
    "        - zeroth_order\n",
    "        - first_order[0][:, np.newaxis, np.newaxis, np.newaxis]\n",
    "        - first_order[1][np.newaxis, :, np.newaxis, np.newaxis]\n",
    "        - first_order[2][np.newaxis, np.newaxis, :, np.newaxis]\n",
    "        - second_order[(0, 1)][:, :, np.newaxis, np.newaxis]\n",
    "        - second_order[(0, 2)][:, np.newaxis, :, np.newaxis]\n",
    "        - second_order[(1, 2)][np.newaxis, :, :, np.newaxis]\n",
    "    )\n",
    "\n",
    "    return zeroth_order, first_order, second_order, third_order\n",
    "\n",
    "# Usage\n",
    "ngram_n = 3\n",
    "epoch = 53_000\n",
    "mm_array = np.memmap(\n",
    "    f'ngram_{ngram_n}_outputs_epoch_{epoch}.npy',\n",
    "    dtype='float32', \n",
    "    mode='r',\n",
    "    shape=(512, 512, 512, 512)\n",
    ")\n",
    "zeroth_order, first_order, second_order, third_order = efron_stein_decomposition(mm_array)\n",
    "\n",
    "# Analysis\n",
    "print(f\"Zeroth-order effect: {zeroth_order}\")\n",
    "for i in range(3):\n",
    "    print(f\"First-order effect (dim {i}) mean magnitude: {np.abs(first_order[i]).mean()}\")\n",
    "for (i, j) in second_order:\n",
    "    print(f\"Second-order effect (dims {i},{j}) mean magnitude: {np.abs(second_order[(i,j)]).mean()}\")\n",
    "print(f\"Third-order effect mean magnitude: {np.abs(third_order).mean()}\")\n",
    "\n",
    "# Variance explained\n",
    "total_var = np.var(mm_array)\n",
    "first_var = sum(np.var(eff) for eff in first_order.values())\n",
    "second_var = sum(np.var(eff) for eff in second_order.values())\n",
    "third_var = np.var(third_order)\n",
    "\n",
    "print(\"\\nVariance explained:\")\n",
    "print(f\"First order: {first_var / total_var * 100:.2f}%\")\n",
    "print(f\"Second order: {second_var / total_var * 100:.2f}%\")\n",
    "print(f\"Third order: {third_var / total_var * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9524f22-e36d-4f9a-8b5a-28c92446374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroth_order = data.mean(axis=0)\n",
    "zeroth_order = torch.from_numpy(zeroth_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc77241-a2ff-4fa1-92ae-69f81da34ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78f441a-fca1-4dfc-a51b-ee91dab27d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 512/512 [06:47<00:00,  1.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 512/512 [08:39<00:00,  1.01s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 512/512 [17:22<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "#zeroth_order = torch.from_numpy(data.mean(axis=0))\n",
    "ngrams = torch.cartesian_prod(torch.arange(512), torch.arange(512), torch.arange(512))\n",
    "first_order = {}\n",
    "for pos in [0, 1, 2]:\n",
    "    effects = []\n",
    "    for token in trange(512):\n",
    "        indices = torch.argwhere(ngrams[:, pos] == token).squeeze().numpy()\n",
    "        val = torch.from_numpy(data[indices]).mean(dim=0) - zeroth_order\n",
    "        effects.append(val)\n",
    "    first_order[pos] = torch.stack(effects, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72aa689c-96ed-404f-9a51-cdbd31eabbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "efron_stein = {'zeroth': zeroth_order, 'first': first_order}\n",
    "torch.save(efron_stein, 'partial_efron_stein.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3df0e053-ce14-4e7c-bb1a-577670bf79d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reconstruction error: 2.384185791015625e-07\n",
      "Zeroth order effect: -0.06464104354381561\n",
      "First order effects:\n",
      "Position 0:\n",
      "tensor([ 0.4549,  0.1557, -0.0581, -0.2491, -0.3034])\n",
      "Position 1:\n",
      "tensor([ 0.7062, -0.5175, -0.7466,  0.7363, -0.1783])\n",
      "Second order effect:\n",
      " tensor([[ 0.3490,  1.0507, -0.4164, -0.0022, -0.9812],\n",
      "        [-0.1368,  0.3021, -1.6845,  0.1970,  1.3221],\n",
      "        [-0.1967, -0.2287,  1.2454, -0.9075,  0.0874],\n",
      "        [ 0.5951, -0.8946,  0.8644,  1.2440, -1.8088],\n",
      "        [-0.6106, -0.2295, -0.0089, -0.5314,  1.3804]])\n",
      "\n",
      "Variance explained:\n",
      "First order: 45.22%\n",
      "Second order: 62.31%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "\n",
    "def generate_random_function_2d(n_tokens=5):\n",
    "    ngrams = torch.cartesian_prod(torch.arange(n_tokens), torch.arange(n_tokens))\n",
    "    fn = torch.randn(n_tokens ** 2)\n",
    "    return ngrams, fn\n",
    "\n",
    "def compute_efron_stein_2d(ngrams, fn, n_tokens=5):\n",
    "    # 0th order effect (mean)\n",
    "    zeroth_order = fn.mean()\n",
    "\n",
    "    # 1st order effects\n",
    "    first_order = {}\n",
    "    for pos in range(2):\n",
    "        first_order[pos] = torch.stack([\n",
    "            fn[ngrams[:, pos] == token].mean() - zeroth_order\n",
    "            for token in range(n_tokens)\n",
    "        ])\n",
    "\n",
    "    # 2nd order effect\n",
    "    second_order = torch.zeros((n_tokens, n_tokens))\n",
    "    for i in range(n_tokens):\n",
    "        for j in range(n_tokens):\n",
    "            mask = (ngrams[:, 0] == i) & (ngrams[:, 1] == j)\n",
    "            second_order[i, j] = (\n",
    "                fn[mask]  # This is a single value for 2D case\n",
    "                - zeroth_order\n",
    "                - first_order[0][i]\n",
    "                - first_order[1][j]\n",
    "            )\n",
    "\n",
    "    return zeroth_order, first_order, second_order\n",
    "\n",
    "def reconstruct_function_2d(zeroth_order, first_order, second_order, n_tokens=5):\n",
    "    reconstructed = torch.zeros(n_tokens, n_tokens)\n",
    "    for i in range(n_tokens):\n",
    "        for j in range(n_tokens):\n",
    "            reconstructed[i, j] = (\n",
    "                zeroth_order\n",
    "                + first_order[0][i]\n",
    "                + first_order[1][j]\n",
    "                + second_order[i, j]\n",
    "            )\n",
    "    return reconstructed.view(-1)\n",
    "\n",
    "# Generate random function\n",
    "n_tokens = 5\n",
    "ngrams, fn = generate_random_function_2d(n_tokens)\n",
    "\n",
    "# Compute Efron-Stein decomposition\n",
    "zeroth_order, first_order, second_order = compute_efron_stein_2d(ngrams, fn, n_tokens)\n",
    "\n",
    "# Reconstruct the function\n",
    "reconstructed_fn = reconstruct_function_2d(zeroth_order, first_order, second_order, n_tokens)\n",
    "\n",
    "# Verify reconstruction\n",
    "error = torch.abs(fn - reconstructed_fn).max()\n",
    "print(f\"Max reconstruction error: {error.item()}\")\n",
    "assert torch.allclose(fn, reconstructed_fn, atol=1e-6), \"Reconstruction failed!\"\n",
    "\n",
    "# Print components\n",
    "print(f\"Zeroth order effect: {zeroth_order.item()}\")\n",
    "print(\"First order effects:\")\n",
    "for pos in range(2):\n",
    "    print(f\"Position {pos}:\\n{first_order[pos]}\")\n",
    "print(\"Second order effect:\\n\", second_order)\n",
    "\n",
    "# Analyze relative importance\n",
    "total_var = torch.var(fn)\n",
    "first_var = sum(torch.var(eff) for eff in first_order.values())\n",
    "second_var = torch.var(second_order)\n",
    "\n",
    "print(\"\\nVariance explained:\")\n",
    "print(f\"First order: {first_var / total_var * 100:.2f}%\")\n",
    "print(f\"Second order: {second_var / total_var * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5fd0b3fe-528f-4df1-a502-c44ccc3d5534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reconstruction error: 2.384185791015625e-07\n",
      "Zeroth order effect: 0.16686588525772095\n",
      "First order effects:\n",
      "Position 0:\n",
      "tensor([ 0.4146,  0.2782, -0.3489, -0.0337, -0.3103])\n",
      "Position 1:\n",
      "tensor([ 0.1551,  0.2297, -0.0873, -0.0148, -0.2827])\n",
      "Position 2:\n",
      "tensor([ 0.2148,  0.3035, -0.3975, -0.0563, -0.0646])\n",
      "Second order effects:\n",
      "Positions 0, 1:\n",
      "tensor([[-0.2600,  0.2227,  0.2253,  0.3525, -0.5405],\n",
      "        [-0.4240,  0.3028,  0.1430,  0.4904, -0.5122],\n",
      "        [ 0.9624, -0.6138,  0.4313, -0.3964, -0.3835],\n",
      "        [ 0.0895,  0.0603, -0.6111,  0.3487,  0.1126],\n",
      "        [-0.3678,  0.0280, -0.1885, -0.7952,  1.3235]])\n",
      "Positions 0, 2:\n",
      "tensor([[-0.0020,  0.2337, -0.0996, -0.0786, -0.0536],\n",
      "        [-0.1320, -0.6032, -0.2954,  0.6642,  0.3663],\n",
      "        [-0.0080, -0.0310,  0.1330,  0.2046, -0.2986],\n",
      "        [-0.2261,  0.1516,  0.0728, -0.2234,  0.2251],\n",
      "        [ 0.3681,  0.2489,  0.1892, -0.5668, -0.2393]])\n",
      "Positions 1, 2:\n",
      "tensor([[ 0.4966, -0.4041, -0.2700,  0.1109,  0.0666],\n",
      "        [ 0.2455,  0.6391,  0.0093, -0.4893, -0.4046],\n",
      "        [-0.7048,  0.0209,  0.0664,  0.5617,  0.0558],\n",
      "        [-0.2897, -0.0885,  0.0355,  0.0451,  0.2976],\n",
      "        [ 0.2523, -0.1673,  0.1587, -0.2284, -0.0154]])\n",
      "Third order effect shape: torch.Size([5, 5, 5])\n",
      "\n",
      "Variance explained:\n",
      "First order: 22.15%\n",
      "Second order: 42.77%\n",
      "Third order: 40.74%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "\n",
    "def generate_random_function_3d(n_tokens=5):\n",
    "    ngrams = torch.cartesian_prod(torch.arange(n_tokens), torch.arange(n_tokens), torch.arange(n_tokens))\n",
    "    fn = torch.randn(n_tokens ** 3)\n",
    "    return ngrams, fn\n",
    "\n",
    "def compute_efron_stein_3d(ngrams, fn, n_tokens=5):\n",
    "    # 0th order effect (mean)\n",
    "    zeroth_order = fn.mean()\n",
    "\n",
    "    # 1st order effects\n",
    "    first_order = {}\n",
    "    for pos in range(3):\n",
    "        first_order[pos] = torch.stack([\n",
    "            fn[ngrams[:, pos] == token].mean() - zeroth_order\n",
    "            for token in range(n_tokens)\n",
    "        ])\n",
    "\n",
    "    # 2nd order effects\n",
    "    second_order = {}\n",
    "    for pos1, pos2 in itertools.combinations(range(3), 2):\n",
    "        effects = torch.zeros((n_tokens, n_tokens))\n",
    "        for token1 in range(n_tokens):\n",
    "            for token2 in range(n_tokens):\n",
    "                mask = (ngrams[:, pos1] == token1) & (ngrams[:, pos2] == token2)\n",
    "                effects[token1, token2] = (\n",
    "                    fn[mask].mean() \n",
    "                    - zeroth_order\n",
    "                    - first_order[pos1][token1] \n",
    "                    - first_order[pos2][token2]\n",
    "                )\n",
    "        second_order[(pos1, pos2)] = effects\n",
    "\n",
    "    # 3rd order effect\n",
    "    third_order = torch.zeros((n_tokens, n_tokens, n_tokens))\n",
    "    for i in range(n_tokens):\n",
    "        for j in range(n_tokens):\n",
    "            for k in range(n_tokens):\n",
    "                mask = (ngrams[:, 0] == i) & (ngrams[:, 1] == j) & (ngrams[:, 2] == k)\n",
    "                third_order[i, j, k] = (\n",
    "                    fn[mask].item()  # This is a single value for 3D case\n",
    "                    - zeroth_order\n",
    "                    - first_order[0][i] - first_order[1][j] - first_order[2][k]\n",
    "                    - second_order[(0, 1)][i, j] - second_order[(0, 2)][i, k] - second_order[(1, 2)][j, k]\n",
    "                )\n",
    "\n",
    "    return zeroth_order, first_order, second_order, third_order\n",
    "\n",
    "def reconstruct_function_3d(zeroth_order, first_order, second_order, third_order, n_tokens=5):\n",
    "    reconstructed = torch.zeros(n_tokens, n_tokens, n_tokens)\n",
    "    for i in range(n_tokens):\n",
    "        for j in range(n_tokens):\n",
    "            for k in range(n_tokens):\n",
    "                reconstructed[i, j, k] = (\n",
    "                    zeroth_order\n",
    "                    + first_order[0][i] + first_order[1][j] + first_order[2][k]\n",
    "                    + second_order[(0, 1)][i, j] + second_order[(0, 2)][i, k] + second_order[(1, 2)][j, k]\n",
    "                    + third_order[i, j, k]\n",
    "                )\n",
    "    return reconstructed.view(-1)\n",
    "\n",
    "# Generate random function\n",
    "n_tokens = 5\n",
    "ngrams, fn = generate_random_function_3d(n_tokens)\n",
    "\n",
    "# Compute Efron-Stein decomposition\n",
    "zeroth_order, first_order, second_order, third_order = compute_efron_stein_3d(ngrams, fn, n_tokens)\n",
    "\n",
    "# Reconstruct the function\n",
    "reconstructed_fn = reconstruct_function_3d(zeroth_order, first_order, second_order, third_order, n_tokens)\n",
    "\n",
    "# Verify reconstruction\n",
    "error = torch.abs(fn - reconstructed_fn).max()\n",
    "print(f\"Max reconstruction error: {error.item()}\")\n",
    "assert torch.allclose(fn, reconstructed_fn, atol=1e-6), \"Reconstruction failed!\"\n",
    "\n",
    "# Print components\n",
    "print(f\"Zeroth order effect: {zeroth_order.item()}\")\n",
    "print(\"First order effects:\")\n",
    "for pos in range(3):\n",
    "    print(f\"Position {pos}:\\n{first_order[pos]}\")\n",
    "print(\"Second order effects:\")\n",
    "for (pos1, pos2), effect in second_order.items():\n",
    "    print(f\"Positions {pos1}, {pos2}:\\n{effect}\")\n",
    "print(\"Third order effect shape:\", third_order.shape)\n",
    "\n",
    "# Analyze relative importance\n",
    "total_var = torch.var(fn)\n",
    "first_var = sum(torch.var(eff) for eff in first_order.values())\n",
    "second_var = sum(torch.var(eff.flatten()) for eff in second_order.values())\n",
    "third_var = torch.var(third_order)\n",
    "\n",
    "print(\"\\nVariance explained:\")\n",
    "print(f\"First order: {first_var / total_var * 100:.2f}%\")\n",
    "print(f\"Second order: {second_var / total_var * 100:.2f}%\")\n",
    "print(f\"Third order: {third_var / total_var * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8e15cc4-a610-4eb6-ad1b-06fe233fef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 5\n",
    "ngrams = torch.cartesian_prod(torch.arange(n_tokens), torch.arange(n_tokens), torch.arange(n_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b3470f4-6be4-4a4d-bb5f-39bcbacb8364",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.randn(n_tokens ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5a28d65-777e-4dca-822e-864e47be68d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "view1 = torch.stack([f[ngrams[:, 1] == i] for i in range(n_tokens)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c113efb-857f-40c0-9dfc-225f39d9e546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  0,   1,   2,   3,   4,  25,  26,  27,  28,  29,  50,  51,  52,  53,\n",
       "          54,  75,  76,  77,  78,  79, 100, 101, 102, 103, 104]),\n",
       " tensor([  5,   6,   7,   8,   9,  30,  31,  32,  33,  34,  55,  56,  57,  58,\n",
       "          59,  80,  81,  82,  83,  84, 105, 106, 107, 108, 109]),\n",
       " tensor([ 10,  11,  12,  13,  14,  35,  36,  37,  38,  39,  60,  61,  62,  63,\n",
       "          64,  85,  86,  87,  88,  89, 110, 111, 112, 113, 114]),\n",
       " tensor([ 15,  16,  17,  18,  19,  40,  41,  42,  43,  44,  65,  66,  67,  68,\n",
       "          69,  90,  91,  92,  93,  94, 115, 116, 117, 118, 119]),\n",
       " tensor([ 20,  21,  22,  23,  24,  45,  46,  47,  48,  49,  70,  71,  72,  73,\n",
       "          74,  95,  96,  97,  98,  99, 120, 121, 122, 123, 124])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.argwhere(ngrams[:, 1] == i).squeeze() for i in range(n_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f4f0228-8b54-43cb-9f3e-1f57e04bdb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8750,  1.4016,  0.0270, -0.4332,  0.0133,  0.6124, -2.0095,  0.6408,\n",
       "         -1.7025, -1.5452, -0.8682,  1.1706, -1.4370, -1.2018, -1.9680, -1.5157,\n",
       "          0.9573, -0.1997,  0.4207, -2.7122, -0.5849, -0.5995,  0.7393, -0.9333,\n",
       "          0.1884],\n",
       "        [ 1.2517, -0.9314,  0.5711,  0.0932,  1.0021, -2.1479,  0.2673, -0.7387,\n",
       "          0.3344, -0.7081, -0.2440,  1.8040,  0.7881, -2.1420,  0.3635, -1.8689,\n",
       "         -0.1884, -1.9096,  0.0948, -0.6922,  0.7987, -0.9315, -1.4617,  1.0115,\n",
       "         -0.3192],\n",
       "        [ 1.0524,  0.6389,  0.6834,  1.4823, -0.2306,  0.8106, -1.0035, -2.0879,\n",
       "          0.6985, -1.1134,  0.4746, -0.8444, -0.7959, -0.1769, -0.0194, -0.5012,\n",
       "          1.8367,  0.8992,  1.0352,  0.9864,  1.7449,  0.2280, -1.4442, -0.5730,\n",
       "         -1.7163],\n",
       "        [ 0.2613,  0.9135, -0.1667, -1.3014, -0.0527,  0.1181,  1.0481,  0.8787,\n",
       "         -0.5568,  1.0012, -1.1522,  0.0993,  0.1824, -2.1522,  0.8663,  0.0649,\n",
       "          0.4856,  0.9354, -1.2093, -1.0450, -1.7064,  0.7601, -0.0156, -0.0317,\n",
       "          1.3650],\n",
       "        [-1.1993, -1.4878, -0.8370, -1.3492, -0.2558,  0.3184, -0.1197,  0.6530,\n",
       "         -1.5452, -1.1367,  0.8946, -0.6888, -0.4893,  0.0188, -0.3551,  0.0803,\n",
       "          0.9823, -0.4720, -0.2088, -0.8224,  0.5281, -0.1661,  1.0081,  0.4085,\n",
       "         -1.2301]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.reshape(5, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee85c68-6727-4cc6-9e5e-6325be79a305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
