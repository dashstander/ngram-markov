<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dashiell Stander">

<title>Methods to Analyze How Transformers Learn N-Grams</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="methods_files/libs/clipboard/clipboard.min.js"></script>
<script src="methods_files/libs/quarto-html/quarto.js"></script>
<script src="methods_files/libs/quarto-html/popper.min.js"></script>
<script src="methods_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="methods_files/libs/quarto-html/anchor.min.js"></script>
<link href="methods_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="methods_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="methods_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="methods_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="methods_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Methods to Analyze How Transformers Learn N-Grams</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Dashiell Stander </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="introduction" class="level1">
<h1>Introduction</h1>
</section>
<section id="methods-for-analyzing-transformers-and-n-grams" class="level1">
<h1>Methods for Analyzing Transformers and N-Grams</h1>
<section id="n-grams-and-the-de-bruijn-graph" class="level2">
<h2 class="anchored" data-anchor-id="n-grams-and-the-de-bruijn-graph">N-Grams and the de Bruijn Graph</h2>
<p>The <a href="https://en.wikipedia.org/wiki/De_Bruijn_graph">de Bruijn graph</a> <span class="math inline">\(B_{m,n}\)</span> is a directed graph, where each vertex <span class="math inline">\(v \in V\)</span> represents an <span class="math inline">\(n\)</span>-tuple of <span class="math inline">\(m\)</span> distinct characters <span class="math inline">\(C\)</span>. We can think of an <span class="math inline">\(n\)</span>-tuple in a de Bruijn graph as a queue that always has <span class="math inline">\(n\)</span> elements. New characters from the vocabulary added to the right hand side and popped off the left hand side. For a de Bruijn graph with <span class="math inline">\((n-1)\)</span>-element tuples, there is an edge if we can get from one set of characters to another by adding an element on the right and removing the left-most element.</p>
<p>Consider <span class="math inline">\(B_{3,2}\)</span>, shown in <a href="#fig-b32">Figure&nbsp;1</a> on the vocabulary <span class="math inline">\(\{0, 1\}\)</span>. There is a directed edge from node <span class="math inline">\((t_1, t_2, t_3) \mapsto (s_1, s_2, s_3)\)</span> if <span class="math inline">\(t_2 = s_1\)</span> and <span class="math inline">\(t_3 = s_2\)</span>. So for <span class="math inline">\(B_{3, 2}\)</span> there are edges <span class="math inline">\((0, 0, 1) \mapsto (0, 1, 0)\)</span> and <span class="math inline">\((0, 1, 0) \mapsto (1, 0, 1)\)</span>, but no edge from <span class="math inline">\((0, 0, 1)\)</span> to <span class="math inline">\((1, 0, 1)\)</span>.</p>
<p>We can consider a text sequence as a <em>walk</em> on the de Bruijn graph. The sequence <code>aaabacabac</code> maps to <code>aa -&gt; aa -&gt; ab -&gt; ba -&gt; ac -&gt; ca -&gt; ab -&gt; ba -&gt; ac</code>. This is the same as the sequence of bigrams (2-grams) and in general a sequence of <span class="math inline">\(n\)</span>-grams can be encoded as a walk on <span class="math inline">\(B_{m, n}\)</span>. There is a directed edge between one node and another precisely when the next node can be the next <span class="math inline">\(n\)</span>-gram. We can’t map <code>ab -&gt; ac</code> because that would “change history” and make it such that <code>b</code> had never come after <code>a</code>.</p>
<p>Note that as <span class="math inline">\(n\)</span> increases, <span class="math inline">\(B_{n,m}\)</span> gets increasingly sparse–there are <span class="math inline">\(m^{n}\)</span> nodes, but each node only ever has <span class="math inline">\(m\)</span> in-going and <span class="math inline">\(m\)</span> out-going edges.</p>
<div class="cell" width="100" data-execution_count="19">
<div class="cell-output cell-output-display">
<div id="fig-b32" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="methods_files/figure-html/fig-b32-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: The de Bruijn graph <span class="math inline">\(B_{3, 2}\)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The <span class="math inline">\(n\)</span>-gram distribution <span class="math inline">\(\nu_n\)</span> for a given corpus of sequences is a probability distribution over the “next” character given the previous <span class="math inline">\(n-1\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> We can view the <span class="math inline">\(n\)</span>-gram probability distribution as a function on the associated de Bruijn graph, <span class="math inline">\(G_{n}: B_{n-1, m} \rightarrow \mathbb{R}^n\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> For each vertex <span class="math inline">\(v \in B_{n-1, m}\)</span>: <span class="math display">\[ \nu_{n}(v) = p(x_{t+1} | x_{t} = v_{n-1}, \dots, x_{t-n+1} = v_{1})\]</span></p>
<p>As we increase <span class="math inline">\(n\)</span>, <span class="math inline">\(\nu_{n}\)</span> becomes a better and better representation of the ground truth distribution for a corpus. Our basic hypotheses are that: 1. A neural network will learn something close to <span class="math inline">\(\nu_{n}\)</span> (for perhaps large <span class="math inline">\(n\)</span>). 2. We will be able to analyze and interpret a neural network by comparing it to <span class="math inline">\(\nu_{n}\)</span>.</p>
</section>
<section id="establishing-the-connection-between-a-neural-network-and-nu_n" class="level2">
<h2 class="anchored" data-anchor-id="establishing-the-connection-between-a-neural-network-and-nu_n">Establishing the Connection Between a Neural Network and <span class="math inline">\(\nu_{n}\)</span></h2>
<p>Before proceeding with analyzing a neural model with respect to the <span class="math inline">\(n\)</span>-gram distribution we need to establish that the distribution the model gives on sequences is sufficiently similar to an <span class="math inline">\(n\)</span>-gram model. Additionally an <span class="math inline">\(n\)</span>-gram distributions assume that the the distribution of the next character is <em>fixed</em> conditional on the prior <span class="math inline">\(n-1\)</span> characters.</p>
<section id="do-transformer-models-learn-n-grams" class="level3">
<h3 class="anchored" data-anchor-id="do-transformer-models-learn-n-grams">Do Transformer Models Learn N-Grams?</h3>
<p>The simplest way to check whether or not a transformer is learning an <span class="math inline">\(n\)</span>-gram distribution <span class="math inline">\(\nu_n\)</span> is to just compare the transformer’s distribution to <span class="math inline">\(\nu_n\)</span>. There are many divergences on discrete probability spaces, but one of the most important is the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Liebler (KL) divergence</a>. For two measures <span class="math inline">\(P, Q\)</span> on a discrete probability space <span class="math inline">\(\chi\)</span>, the KL divergence <span class="math inline">\(D_{KL}\)</span> is:</p>
<p><span class="math display">\[ D_{KL}(P || Q) = \sum_{x \in \chi} P(x)\log[\frac{P(x)}{Q(x)}]\]</span></p>
<p>In our case the sample space is <span class="math inline">\(n\)</span>-tuples of characters <span class="math inline">\(C^n\)</span>. We can get a good estimate of the KL-divergence by just sampling <span class="math inline">\(c \in C^{n-1}\)</span> uniformly from the corpus and calculating the KL divergence of the <em>conditional</em> distributions: <span class="math inline">\(\mathbb{E}_{c \in C^{n-1}}[D_{KL}(T(\cdot | c) || \nu_{n}(\cdot | c)]\)</span></p>
<p>We have done these calculations for transformer models trained on <a href="https://huggingface.co/datasets/roneneldan/TinyStories">TinyStories</a>, which you can see in <a href="#fig-ngram-kl">Figure&nbsp;2</a></p>
<div id="fig-ngram-kl" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ngram_kl_eight_layers.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: KL Divergence between an 8 layer transformer and the N-Gram distribution over the course of training.</figcaption><p></p>
</figure>
</div>
</section>
<section id="how-much-do-transformer-models-depend-on-prior-context" class="level3">
<h3 class="anchored" data-anchor-id="how-much-do-transformer-models-depend-on-prior-context">How Much Do Transformer Models Depend on Prior Context?</h3>
<p>An assumption in an <span class="math inline">\(n\)</span>-gram probability distribution is that probability of token is dependent <em>only</em> on the previous <span class="math inline">\(n-1\)</span> tokens: $p(c_{t+1} = c) = p(c_{t+1} = p(c | c_t, …, c_{t-n+1}) $. All of the prior context does not matter at all. This is obviously a simplification for “true” language, but for <span class="math inline">\(n\)</span>-grams to be a useful lens we need to confirm the extent to which it is true for neural language models.</p>
<p>There are two ways</p>
</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The <span class="math inline">\(n\)</span>th character is the one being predicted.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The de Bruijn graph on <span class="math inline">\(n-1\)</span> vertices because the distribution is conditioned on <span class="math inline">\(n-1\)</span> characters.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>